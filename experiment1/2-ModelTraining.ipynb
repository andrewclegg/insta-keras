{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "\n",
    "This notebook defines and trains a Keras network to predict, for each order, which of the items in the user's _previous_ order will reappear in it.\n",
    "\n",
    "This is slightly different from the goal of the Kaggle contest, which is to predict which items from _any_ point in the user's past history will reappear in this order.\n",
    "\n",
    "Make sure you run the [Data Preparation](1-DataPrep.ipynb) notebook before this one.\n",
    "\n",
    "Training will be performed on a GPU automatically if your machine has one, or CPU otherwise (assuming you haven't messed with Keras's configuration).\n",
    "\n",
    "## Problem definition\n",
    "\n",
    "Conceptually, we're trying to predict a number of items for each order -- a multilabel classification problem.\n",
    "\n",
    "However, since the candidate items for each order come from a restricted set (the _n_ items in the previous order), this decomposes nicely into _n_ individual binary classification problems. This is generally an easier problem to solve, especially when _n_ is much smaller than the total number of items in the data.\n",
    "\n",
    "So, each training (or test) instance is actually an item in an order, and the label is 1 if that item also appears in the next order, and 0 otherwise.\n",
    "\n",
    "This means each order yields _n_ separate training instances, one for each item it contains.\n",
    "\n",
    "## Network structure\n",
    "\n",
    "The inputs to the network, for each training instance, are:\n",
    "\n",
    "* (1) The order that this instance came from\n",
    " * This remains the same for all the _n_ instances from this order\n",
    "* (2) The items in that order which were themselves reorders\n",
    " * This also remains the same for all instances from this order\n",
    "* (3) The user\n",
    " * This also remains the same for all instances from this order\n",
    "* (4) The item itself\n",
    " * This is the only input that differs between instances from the same order\n",
    "\n",
    "The label, for each instance, is a 1 or 0 indicating whether or not it reappears in the user's next order.\n",
    "\n",
    "All items are represented by an _embedding_, i.e. a vector of floats. These values are learnt during training, along with all the other weights of the network, and items which appear in similar contexts will result in similar embeddings.\n",
    "\n",
    "A collection of items (i.e. (1) and (2)) is represented by getting the embeddings for those items and simply adding them together, so an empty collection is just a vector of zeros.\n",
    "\n",
    "Each user is also represented by an embedding, again learnt during training.\n",
    "\n",
    "The final input to the network, then, is a concatenation of four vectors:\n",
    "\n",
    "* The order vector (sum of item embeddings in order)\n",
    "* The reorder vector (sum of reordered item embeddings in order)\n",
    "* The current item's embedding\n",
    "* The current user's embedding\n",
    "\n",
    "Or more visually:\n",
    "\n",
    "```\n",
    "|---Order-Vector-6---||--Reorder-Vector-6--||---Item-Vector-13---||---User-Vector-22---|\n",
    "|---Order-Vector-6---||--Reorder-Vector-6--||---Item-Vector-43---||---User-Vector-22---|\n",
    "|---Order-Vector-6---||--Reorder-Vector-6--||---Item-Vector-91---||---User-Vector-22---|\n",
    "...\n",
    "|---Order-Vector-9---||--Reorder-Vector-9--||---Item-Vector-10---||---User-Vector-71---|\n",
    "|---Order-Vector-9---||--Reorder-Vector-9--||---Item-Vector-13---||---User-Vector-71---|\n",
    "...\n",
    "```\n",
    "\n",
    "These vectors are fed through two consecutive fully-connected layers, each the same width, and then to a single output node which predicts the label. This is scored against the true label via [cross-entropy loss](https://jamesmccaffrey.wordpress.com/2013/11/05/why-you-should-use-cross-entropy-error-instead-of-classification-error-or-mean-squared-error-for-neural-network-classifier-training/).\n",
    "\n",
    "## Evaluation metrics\n",
    "\n",
    "Before training, 5000 randomly-selected order pairs will be set aside as validation data. After each training epoch -- i.e. each pass through the training data -- we'll evaluate the model's predictive power over the validation set, reporting [precision, recall and F1 score](https://en.wikipedia.org/wiki/Precision_and_recall) (balanced F-measure) averaged over these orders.\n",
    "\n",
    "N.B. This isn't necessarily a perfect predictor of how well the model will do in production, as the training data can potentially contain order pairs that appear chronologically later in the input data than order pairs in the validation data. This is a violation of the \"no time machines\" rule. But, it's convenient for demonstration purposes. A thorough evaluation would involve a held-out test set consisting of each user's most recent order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras as k\n",
    "import sklearn as sk\n",
    "import tensorflow as tf\n",
    "import h5py\n",
    "import os\n",
    "\n",
    "h5_dir = 'h5/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reopen saved datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datafile = h5py.File(os.path.join(h5_dir, 'training_data.h5'), 'r')\n",
    "orders_dataset = datafile['orders']\n",
    "reorders_dataset = datafile['reorders']\n",
    "users_dataset = datafile['users']\n",
    "items_dataset = datafile['items']\n",
    "labels_dataset = datafile['labels']\n",
    "num_rows = datafile['num_rows'][0]\n",
    "biggest_order_size = datafile['biggest_order_size'][0]\n",
    "max_product_id = datafile['max_product_id'][0]\n",
    "max_user_id = datafile['max_user_id'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network helpers\n",
    "\n",
    "Not all Keras layers support masking (e.g. ignoring product ID 0 which is effectively 'null' in our data).\n",
    "\n",
    "So we create a new type of layer that removes masked-out values before passing the data on to the next layer. See: https://github.com/fchollet/keras/issues/2728\n",
    "\n",
    "Also we create a couple of helper functions to let us sum all the vectors coming from a previous layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "from keras.engine.topology import Layer\n",
    "\n",
    "class ZeroMaskedEntries(Layer):\n",
    "    \"\"\"\n",
    "    This layer is called after an Embedding layer.\n",
    "    It zeros out all of the masked-out embeddings.\n",
    "    It also swallows the mask without passing it on.\n",
    "    You can change this to default pass-on behavior as follows:\n",
    "\n",
    "    def compute_mask(self, x, mask=None):\n",
    "        if not self.mask_zero:\n",
    "            return None\n",
    "        else:\n",
    "            return K.not_equal(x, 0)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        self.support_mask = True\n",
    "        super(ZeroMaskedEntries, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.output_dim = input_shape[1]\n",
    "        self.repeat_dim = input_shape[2]\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        mask = K.cast(mask, 'float32')\n",
    "        mask = K.repeat(mask, self.repeat_dim)\n",
    "        mask = K.permute_dimensions(mask, (0, 2, 1))\n",
    "        return x * mask\n",
    "\n",
    "    def compute_mask(self, input_shape, input_mask=None):\n",
    "        return None\n",
    "\n",
    "def sum_layer_output_shape(input_shape):\n",
    "    shape = list(input_shape)\n",
    "    assert len(shape) == 3 \n",
    "    return (shape[0], shape[2])\n",
    "\n",
    "def sum_layer(x):\n",
    "  return K.sum(x, axis=1, keepdims=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â Network structure\n",
    "\n",
    "**TODO** Fill this in, maybe with a diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "order_input (InputLayer)         (None, 145)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "reorder_input (InputLayer)       (None, 145)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "item_input (InputLayer)          (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)          multiple              1242225     order_input[0][0]                \n",
      "                                                                   reorder_input[0][0]              \n",
      "                                                                   item_input[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "user_input (InputLayer)          (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "zero_masked_entries_3 (ZeroMaske (None, 145, 25)       0           embedding_3[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "zero_masked_entries_4 (ZeroMaske (None, 145, 25)       0           embedding_3[1][0]                \n",
      "____________________________________________________________________________________________________\n",
      "lambda_6 (Lambda)                (None, 1, 25)         0           embedding_3[2][0]                \n",
      "____________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)          (None, 1, 25)         5155250     user_input[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)                (None, 25)            0           zero_masked_entries_3[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)                (None, 25)            0           zero_masked_entries_4[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)              (None, 25)            0           lambda_6[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)              (None, 25)            0           embedding_4[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)      (None, 100)           0           lambda_4[0][0]                   \n",
      "                                                                   lambda_5[0][0]                   \n",
      "                                                                   flatten_3[0][0]                  \n",
      "                                                                   flatten_4[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "hidden_1 (Dense)                 (None, 100)           10100       concatenate_2[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 100)           0           hidden_1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "hidden_2 (Dense)                 (None, 100)           10100       dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "output (Dense)                   (None, 1)             101         hidden_2[0][0]                   \n",
      "====================================================================================================\n",
      "Total params: 6,417,776\n",
      "Trainable params: 6,417,776\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "user_embedding_size = 50\n",
    "product_embedding_size = 50\n",
    "hidden_1_size = 200\n",
    "hidden_2_size = 200\n",
    "activation = 'elu'\n",
    "\n",
    "dropout = 0.33\n",
    "\n",
    "user_input = k.layers.Input(shape=(1,), name='user_input')\n",
    "order_input = k.layers.Input(shape=(biggest_order_size,), name='order_input')\n",
    "reorder_input = k.layers.Input(shape=(biggest_order_size,), name='reorder_input')\n",
    "item_input = k.layers.Input(shape=(1,), name='item_input')\n",
    "\n",
    "product_embedding = k.layers.Embedding(input_dim=max_product_id + 1,\n",
    "                                       output_dim=product_embedding_size,\n",
    "                                       mask_zero=True)\n",
    "\n",
    "# Sum up the embeddings in the order and reorder sets\n",
    "order_embedding = ZeroMaskedEntries()(product_embedding(order_input))\n",
    "order_embedding_sum = k.layers.Lambda(sum_layer, sum_layer_output_shape)(order_embedding)\n",
    "reorder_embedding = ZeroMaskedEntries()(product_embedding(reorder_input))\n",
    "reorder_embedding_sum = k.layers.Lambda(sum_layer, sum_layer_output_shape)(reorder_embedding)\n",
    "\n",
    "# Hack: Flatten also doesn't support masks, but we don't need to do anything special\n",
    "# The item embeddings will never be masked out, there's always exactly one in each\n",
    "demask = k.layers.Lambda(lambda x: x, output_shape=lambda s:s)\n",
    "item_embedding = k.layers.Flatten()(demask(product_embedding(item_input)))\n",
    "\n",
    "user_embedding = k.layers.Flatten()(k.layers.Embedding(input_dim=max_user_id + 1,\n",
    "                                                       output_dim=user_embedding_size)(user_input))\n",
    "\n",
    "concatenated = k.layers.concatenate([order_embedding_sum,\n",
    "                                     reorder_embedding_sum,\n",
    "                                     item_embedding,\n",
    "                                     user_embedding])\n",
    "\n",
    "hidden_1 = k.layers.Dense(hidden_1_size, activation=activation, name='hidden_1')(concatenated)\n",
    "dropout_1 = k.layers.Dropout(dropout, name='dropout_1')(hidden_1)\n",
    "\n",
    "hidden_2 = k.layers.Dense(hidden_2_size, activation=activation, name='hidden_2')(dropout_1)\n",
    "dropout_2 = k.layers.Dropout(dropout, name='dropout_2')(hidden_2)\n",
    "\n",
    "output = k.layers.Dense(1, activation='sigmoid', name='output')(hidden_2)\n",
    "\n",
    "model = k.models.Model(inputs=[order_input, reorder_input, user_input, item_input], outputs=output)\n",
    "model.compile(optimizer=tf.train.AdamOptimizer(learning_rate=0.01), loss='binary_crossentropy')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "**TODO** Fill in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "num_valid = 5000\n",
    "\n",
    "index = np.arange(num_rows, dtype=np.uint32)\n",
    "np.random.shuffle(index)\n",
    "index_valid = index[:num_valid]\n",
    "index_train = index[num_valid:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_training_data(indices):\n",
    "  indices = np.sort(indices).tolist()\n",
    "  labels_separate = labels_dataset[indices]\n",
    "  labels = np.hstack(labels_separate)\n",
    "  order_lengths = [len(items) for items in labels_separate] # num items in each target bag\n",
    "  orders_separate = orders_dataset[indices]\n",
    "  reorders_separate = reorders_dataset[indices]\n",
    "  orders_data = np.repeat(orders_separate, order_lengths, axis=0)\n",
    "  reorders_data = np.repeat(reorders_separate, order_lengths, axis=0)\n",
    "  user_ids = np.repeat(users_dataset[indices], order_lengths)\n",
    "  item_ids = np.hstack(items_dataset[indices])\n",
    "  item_lengths = [len(x) for x in labels_separate]\n",
    "  return (orders_data, reorders_data, user_ids, item_ids, labels, item_lengths)\n",
    "\n",
    "def validate(model):\n",
    "  loss = model.test_on_batch({'order_input': valid_order_data,\n",
    "                              'reorder_input': valid_reorder_data,\n",
    "                              'item_input': valid_item_ids,\n",
    "                              'user_input': valid_user_ids}, valid_labels)\n",
    "  output = model.predict_on_batch({'order_input': valid_order_data,\n",
    "                                   'reorder_input': valid_reorder_data,\n",
    "                                   'item_input': valid_item_ids,\n",
    "                                   'user_input': valid_user_ids}) # Can we combine these two calls?\n",
    "  preds = np.where(output > 0.5, 1, 0)\n",
    "  preds_split = np.split(preds, valid_split_intervals)[:-1]\n",
    "  precision = np.zeros(num_valid)\n",
    "  recall = np.zeros(num_valid)\n",
    "  for i, (preds, labels) in enumerate(zip(preds_split, valid_labels_split)):\n",
    "    matches = float(sum(preds.T[0] * labels))\n",
    "    if sum(preds) > 0:\n",
    "      precision[i] = matches / sum(preds)\n",
    "    if sum(labels) > 0:\n",
    "      recall[i] = matches / sum(labels)\n",
    "  f1 = np.nan_to_num((2 * precision * recall) / (precision + recall))\n",
    "  return (loss, precision.mean(), recall.mean(), f1.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid_order_data, valid_reorder_data, valid_user_ids, valid_item_ids, valid_labels, valid_lengths = make_training_data(index_valid)\n",
    "valid_split_intervals = np.cumsum(valid_lengths)\n",
    "valid_labels_split = np.split(valid_labels, valid_split_intervals)[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "steps_per_epoch = num_rows // batch_size\n",
    "epochs = 1\n",
    "\n",
    "print(\"Batch size: %d\" % batch_size)\n",
    "print(\"Training examples: %d\" % num_rows)\n",
    "print(\"Batches per epoch: %d\" % steps_per_epoch)\n",
    "\n",
    "def train_data_generator():\n",
    "  for chunk in np.array_split(index_train, steps_per_epoch):\n",
    "    orders_data, reorders_data, user_ids, item_ids, labels, item_lengths = make_training_data(chunk)\n",
    "    yield ({'order_input': orders_data,\n",
    "            'reorder_input': reorders_data,\n",
    "            'item_input': item_ids,\n",
    "            'user_input': user_ids},\n",
    "           {'output': labels})\n",
    "  return\n",
    "\n",
    "# TODO early stopping, checkpointing, learning rate reduction\n",
    "\n",
    "for epoch in range(epochs):\n",
    "  \n",
    "  print(\"Starting epoch %d\" % epoch)\n",
    "  \n",
    "  labels_sample = np.hstack(labels_dataset[np.sort(index_train[0:num_valid]).tolist()])\n",
    "  class_weights = sk.utils.class_weight.compute_class_weight('balanced', [0, 1], labels_sample)\n",
    "  del labels_sample\n",
    "  \n",
    "  model.fit_generator(train_data_generator(), steps_per_epoch=steps_per_epoch, epochs=1, max_q_size=1,\n",
    "                      class_weight={0: class_weights[0], 1: class_weights[1]})\n",
    "  \n",
    "  print(\"Validation: loss = %0.5f, P = %0.5f, R = %0.5f, F = %0.5f\" % validate(model))\n",
    "  print(\"Saving model\")\n",
    "  model.save('models/toy_5.h5')\n",
    "  print(\"Shuffling training data\")\n",
    "  np.random.shuffle(index_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
