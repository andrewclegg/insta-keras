{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "\n",
    "This notebook defines and trains a Keras network to predict, for each order, which of the items in the user's _previous_ order will reappear in it.\n",
    "\n",
    "This is slightly different from the goal of the Kaggle contest, which is to predict which items from _any_ point in the user's past history will reappear in this order.\n",
    "\n",
    "Make sure you run the [Data Preparation](1-DataPrep.ipynb) notebook before this one.\n",
    "\n",
    "Training will be performed on a GPU automatically if your machine has one, or CPU otherwise (assuming you haven't messed with Keras's configuration).\n",
    "\n",
    "## Problem definition\n",
    "\n",
    "Conceptually, we're trying to predict a number of items for each order -- a multilabel classification problem.\n",
    "\n",
    "However, since the candidate items for each order come from a restricted set (the _n_ items in the previous order), this decomposes nicely into _n_ individual binary classification problems. This is generally an easier problem to solve, especially when _n_ is much smaller than the total number of items in the data.\n",
    "\n",
    "So, each training (or test) instance is actually an item in an order, and the label is 1 if that item also appears in the next order, and 0 otherwise.\n",
    "\n",
    "This means each order yields _n_ separate training instances, one for each item it contains.\n",
    "\n",
    "## Network structure\n",
    "\n",
    "The inputs to the network, for each training instance, are:\n",
    "\n",
    "* (1) The order that this instance came from\n",
    " * This remains the same for all the _n_ instances from this order\n",
    "* (2) The items in that order which were themselves reorders\n",
    " * This also remains the same for all instances from this order\n",
    "* (3) The user\n",
    " * This also remains the same for all instances from this order\n",
    "* (4) The item itself\n",
    " * This is the only input that differs between instances from the same order\n",
    "\n",
    "The label, for each instance, is a 1 or 0 indicating whether or not it reappears in the user's next order.\n",
    "\n",
    "All items are represented by an _embedding_, i.e. a vector of floats. These values are learnt during training, along with all the other weights of the network, and items which appear in similar contexts will result in similar embeddings.\n",
    "\n",
    "A collection of items (i.e. (1) and (2)) is represented by getting the embeddings for those items and simply adding them together, so an empty collection is just a vector of zeros.\n",
    "\n",
    "Each user is also represented by an embedding, again learnt during training.\n",
    "\n",
    "The final input to the network, then, is a concatenation of four vectors:\n",
    "\n",
    "* The order vector (sum of item embeddings in order)\n",
    "* The reorder vector (sum of reordered item embeddings in order)\n",
    "* The current item's embedding\n",
    "* The current user's embedding\n",
    "\n",
    "Or more visually (each row is a single training instance):\n",
    "\n",
    "```\n",
    "|---Order-Vector-6---||--Reorder-Vector-6--||---Item-Vector-13---||---User-Vector-22---|\n",
    "|---Order-Vector-6---||--Reorder-Vector-6--||---Item-Vector-43---||---User-Vector-22---|\n",
    "|---Order-Vector-6---||--Reorder-Vector-6--||---Item-Vector-91---||---User-Vector-22---|\n",
    "...\n",
    "|---Order-Vector-9---||--Reorder-Vector-9--||---Item-Vector-10---||---User-Vector-71---|\n",
    "|---Order-Vector-9---||--Reorder-Vector-9--||---Item-Vector-13---||---User-Vector-71---|\n",
    "...\n",
    "```\n",
    "\n",
    "These vectors are fed through a series of fully-connected layers, and then to a single output node which predicts the label. This is scored against the true label via [cross-entropy loss](https://jamesmccaffrey.wordpress.com/2013/11/05/why-you-should-use-cross-entropy-error-instead-of-classification-error-or-mean-squared-error-for-neural-network-classifier-training/). The exact structure of the network is discussed below.\n",
    "\n",
    "## Evaluation metrics\n",
    "\n",
    "Before training, 5000 randomly-selected order pairs will be set aside as validation data. After each training epoch -- i.e. each pass through the training data -- we'll evaluate the model's predictive power over the validation set, reporting [precision, recall and F1 score](https://en.wikipedia.org/wiki/Precision_and_recall) (balanced F-measure) averaged over these orders.\n",
    "\n",
    "N.B. This isn't necessarily a perfect predictor of how well the model will do in production, as the training data can potentially contain order pairs that appear chronologically later in the input data than order pairs in the validation data. This is a violation of the \"no time machines\" rule. But, it's convenient for demonstration purposes. A thorough evaluation would involve a held-out test set consisting of each user's most recent order.\n",
    "\n",
    "## RAM usage\n",
    "\n",
    "Beware! This notebook requires over 4GB of RAM to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras as k\n",
    "import keras.backend as K\n",
    "from keras.engine.topology import Layer\n",
    "import sklearn as sk\n",
    "import tensorflow as tf\n",
    "import h5py\n",
    "import os\n",
    "\n",
    "h5_dir = 'h5/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reopen saved datasets\n",
    "\n",
    "Pull all the data out of HDF5 file and back into memory. This is much faster than doing random access directly on the files, particularly if you're using a cloud server with network-attached storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datafile = h5py.File(os.path.join(h5_dir, 'training_data.h5'), 'r')\n",
    "orders_dataset = datafile['orders'][:]\n",
    "reorders_dataset = datafile['reorders'][:]\n",
    "users_dataset = datafile['users'][:]\n",
    "items_dataset = datafile['items'][:]\n",
    "labels_dataset = datafile['labels'][:]\n",
    "num_rows = datafile['num_rows'][0]\n",
    "biggest_order_size = datafile['biggest_order_size'][0]\n",
    "max_product_id = datafile['max_product_id'][0]\n",
    "max_user_id = datafile['max_user_id'][0]\n",
    "datafile.close()\n",
    "del datafile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network helpers\n",
    "\n",
    "From: https://github.com/fchollet/keras/issues/2728"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ZeroMaskedEntries(k.engine.topology.Layer):\n",
    "    \"\"\"\n",
    "    This layer is called after an Embedding layer.\n",
    "    It zeros out all of the masked-out embeddings.\n",
    "    It also swallows the mask without passing it on.\n",
    "    You can change this to default pass-on behavior as follows:\n",
    "\n",
    "    def compute_mask(self, x, mask=None):\n",
    "        if not self.mask_zero:\n",
    "            return None\n",
    "        else:\n",
    "            return K.not_equal(x, 0)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        self.support_mask = True\n",
    "        super(ZeroMaskedEntries, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.output_dim = input_shape[1]\n",
    "        self.repeat_dim = input_shape[2]\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        mask = K.cast(mask, 'float32')\n",
    "        mask = K.repeat(mask, self.repeat_dim)\n",
    "        mask = K.permute_dimensions(mask, (0, 2, 1))\n",
    "        return x * mask\n",
    "\n",
    "    def compute_mask(self, input_shape, input_mask=None):\n",
    "        return None\n",
    "\n",
    "\n",
    "def mask_aware_mean(x):\n",
    "    # recreate the masks - all zero rows have been masked\n",
    "    mask = K.not_equal(K.sum(K.abs(x), axis=2, keepdims=True), 0)\n",
    "\n",
    "    # number of that rows are not all zeros\n",
    "    n = K.sum(K.cast(mask, 'float32'), axis=1, keepdims=False)\n",
    "    \n",
    "    # compute mask-aware mean of x, or all zeroes if no rows present\n",
    "    x_mean = K.sum(x, axis=1, keepdims=False) / n\n",
    "    x_mean = tf.where(tf.is_nan(x_mean), tf.zeros_like(x_mean), x_mean)\n",
    "    x_mean = tf.verify_tensor_all_finite(x_mean, 'fuck')\n",
    "\n",
    "    return x_mean\n",
    "\n",
    "\n",
    "def mask_aware_mean_output_shape(input_shape):\n",
    "    shape = list(input_shape)\n",
    "    assert len(shape) == 3 \n",
    "    return (shape[0], shape[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â Network structure\n",
    "\n",
    "The following code defines the structure of the neural network, as described in the notes at the top of this file.\n",
    "\n",
    "The constants at the top define the dimensionality of the user and product embeddings, and the hidden layer sizes. The final dimensionality of the concatenated input vector is:\n",
    "\n",
    "`(product_embedding_size * 3) + user_embedding_size`\n",
    "\n",
    "because we have order, reorder and current item vectors, again as described above.\n",
    "\n",
    "Note that there are actually two separate product embedding tables, for the context (previous order and reorder) and the target item. This gives the model more room to manoeuvre. We call these the 'in' and 'out' embeddings like in word2vec. The user embedding is drawn from a different table altogether.\n",
    "\n",
    "The hidden layers are [exponential linear units](https://arxiv.org/abs/1511.07289), with [dropout](http://jmlr.org/papers/v15/srivastava14a.html) to reduce the effects of overfitting. The network is trained using the [ADAM optimizer](https://arxiv.org/abs/1412.6980).\n",
    "\n",
    "### Note\n",
    "\n",
    "When you run the following cell, no data actually gets processed.\n",
    "\n",
    "The Keras API calls here just _define_ the network structure -- think of it like a declarative DSL for describing a network.\n",
    "\n",
    "We'll actually pass data into the network, and evaluate its output, later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Layer size constants\n",
    "user_embedding_size = 20\n",
    "product_embedding_size = 20\n",
    "hidden_1_size = 160\n",
    "hidden_2_size = 160\n",
    "\n",
    "# Don't change this -- number of continuous features\n",
    "cont_features_size = 2\n",
    "\n",
    "# Activation function for the hidden layers\n",
    "activation = 'relu'\n",
    "\n",
    "# Dropout rate for the hidden layers\n",
    "dropout = 0.1\n",
    "\n",
    "# Initial learning rate\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Input layers for the four datasets, and the continuous features\n",
    "\n",
    "user_input = k.layers.Input(shape=(1,), name='user_input')\n",
    "\n",
    "order_input = k.layers.Input(shape=(biggest_order_size,), name='order_input')\n",
    "\n",
    "reorder_input = k.layers.Input(shape=(biggest_order_size,), name='reorder_input')\n",
    "\n",
    "item_input = k.layers.Input(shape=(1,), name='item_input')\n",
    "\n",
    "cont_features_input = k.layers.Input(shape=(cont_features_size,), name='cont_features_input')\n",
    "\n",
    "# Set up the embeddings tables -- these map the order, reorder, user, and\n",
    "# individual item IDs into embeddings\n",
    "# Think of them as lookup tables mapping IDs to vectors of floats\n",
    "\n",
    "# Similar to word2vec, for products we use separate 'in' embeddings\n",
    "# (for the context) and 'out' embeddings (for the item we're trying to predict)\n",
    "\n",
    "product_embedding_in = k.layers.Embedding(\n",
    "  input_dim=max_product_id + 1, output_dim=product_embedding_size,\n",
    "  name='product_embedding_in')\n",
    "\n",
    "product_embedding_out = k.layers.Embedding(\n",
    "  input_dim=max_product_id + 1, output_dim=product_embedding_size,\n",
    "  name='product_embedding_out')\n",
    "\n",
    "# Set up a separate user embedding table\n",
    "\n",
    "user_embedding = k.layers.Embedding(\n",
    "  input_dim=max_user_id + 1, output_dim=user_embedding_size, name='user_embedding')\n",
    "\n",
    "# Sum up the embeddings in the order and reorder sets into order and reorder \n",
    "\n",
    "order_embedding = product_embedding_in(order_input)\n",
    "\n",
    "order_vector = k.layers.Lambda(\n",
    "  mask_aware_mean, mask_aware_mean_output_shape)(order_embedding)\n",
    "\n",
    "reorder_embedding = product_embedding_in(reorder_input)\n",
    "\n",
    "reorder_vector = k.layers.Lambda(\n",
    "  mask_aware_mean, mask_aware_mean_output_shape)(reorder_embedding)\n",
    "\n",
    "# Flatten the single item embedding into a vector\n",
    "\n",
    "item_vector = k.layers.Flatten(name='item_vector')(\n",
    "  product_embedding_out(item_input))\n",
    "\n",
    "# Flatten the single user embedding into a vector\n",
    "\n",
    "user_vector = k.layers.Flatten(name='user_vector')(\n",
    "  user_embedding(user_input))\n",
    "\n",
    "# Concatenate embeddings with explicitly-supplied features\n",
    "\n",
    "full_input_vector = k.layers.concatenate(\n",
    "  [order_vector, reorder_vector, item_vector, user_vector, cont_features_input],\n",
    "  name=\"full_input_vector\")\n",
    "\n",
    "# Define the hidden layers\n",
    "\n",
    "hidden_1 = k.layers.Dense(hidden_1_size, activation=activation, name='hidden_1')(full_input_vector)\n",
    "dropout_1 = k.layers.Dropout(dropout, name='dropout_1')(hidden_1)\n",
    "\n",
    "hidden_2 = k.layers.Dense(hidden_2_size, activation=activation, name='hidden_2')(dropout_1)\n",
    "dropout_2 = k.layers.Dropout(dropout, name='dropout_2')(hidden_2)\n",
    "\n",
    "output = k.layers.Dense(1, activation='sigmoid', name='output')(hidden_2)\n",
    "\n",
    "# Compile the model\n",
    "\n",
    "model = k.models.Model(\n",
    "  inputs=[order_input, reorder_input, user_input, item_input, cont_features_input],\n",
    "  outputs=output)\n",
    "\n",
    "model.compile(\n",
    "  optimizer=k.optimizers.SGD(lr=learning_rate, decay=1e-4), loss='binary_crossentropy')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "The following sections set up the training process.\n",
    "\n",
    "We read the data in batches of 1000 order pairs at a time, although each of these gets expanded into one training instance per item in the earlier order, as described earlier. The label for each instance is whether that item appears in the later order or not.\n",
    "\n",
    "The data is shuffled before training starts, and again at the end of each training epoch, because data with non-random ordering can cause batch-based optimization algorithms to behave in unexpected ways and perform badly.\n",
    "\n",
    "Note that shuffling the data is done indirectly, via a numeric index into the input datasets, rather than by trying to shuffle them in-place but in a synchronized manner. This makes the code much simpler, and is faster too.\n",
    "\n",
    "We set aside 5000 order pairs as the validation set. If you change these, make sure `num_valid` is a multiple of `batch_size` so it doesn't skew the average loss calculation in the `validate` function later.\n",
    "\n",
    "Also, we use a helper function from scikit-learn to calculate weights for the classes, based on the frequencies of these classes in the training data. This is necessary because class 0 (not a reorder) is much more common than class 1 (reorder). Without adjusting for this, the model would just get stuck in a local optimum where it predicts 0 for every item. By reweighting them, we make false negatives (predicting 0s for items that should be 1s) much more costly to the model, so it avoids this trap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "num_valid = 5000\n",
    "assert num_valid % batch_size == 0\n",
    "\n",
    "index = np.arange(num_rows, dtype=np.uint32)\n",
    "np.random.shuffle(index)\n",
    "index_valid = index[:num_valid]\n",
    "index_train = index[num_valid:]\n",
    "\n",
    "class_weights = sk.utils.class_weight.compute_class_weight(\n",
    "  'balanced', [0, 1], np.hstack(labels_dataset[index_train]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define a couple of helper functions.\n",
    "\n",
    "`make_training_data` is called once per batch, on a list of indices into the training data. It retrieves the records corresponding to those indices from the underlying datasets, performs some minor adjustments and returns them as Numpy arrays.\n",
    "\n",
    "`validate` is called once per epoch, i.e. after one whole run through the training data. It calculates some metrics on the validation data -- batch by batch as if it was training data -- and returns them.\n",
    "\n",
    "`validate` is a bit inefficient as it regenerates the input arrays (via `make_training_data`) every time it's called, even though these don't change and could be cached. This would be easy to implement at the cost of higher memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_training_data(indices):\n",
    "  \n",
    "  # Get labels for this batch's instances, and stack them into a single vector\n",
    "  labels_separate = labels_dataset[indices]\n",
    "  labels = np.hstack(labels_separate)\n",
    "  \n",
    "  # Get the arrays of ordered item IDs and reordered item IDs for each order\n",
    "  # (these are already zero-padded to the same length)\n",
    "  orders_separate = orders_dataset[indices]\n",
    "  reorders_separate = reorders_dataset[indices]\n",
    "  \n",
    "  # From the length of each individual label array, we know how many items\n",
    "  # are in each of the orders\n",
    "  order_lengths = [len(items) for items in labels_separate]\n",
    "\n",
    "  # Repeat each order array and each reorder array that many times\n",
    "  orders_data = np.repeat(orders_separate, order_lengths, axis=0)\n",
    "  reorders_data = np.repeat(reorders_separate, order_lengths, axis=0)\n",
    "  \n",
    "  # For the reorder lengths we have to actually count the IDs\n",
    "  reorder_lengths = [np.count_nonzero(reorder) for reorder in reorders_separate]\n",
    "  \n",
    "  # Also repeat each user ID the appropriate number of times\n",
    "  user_ids = np.repeat(users_dataset[indices], order_lengths)\n",
    "  \n",
    "  # We don't need to repeat the item IDs, each training instance corresponds\n",
    "  # to a single item, to we just concatenate them into a single array\n",
    "  item_ids = np.hstack(items_dataset[indices])\n",
    "  \n",
    "  # Calculate continuous features: currently just order size and reorder size\n",
    "  # (both normalized by biggest_order_size), repeated over all items in order\n",
    "  order_size_vec = np.array(order_lengths, dtype=np.float32) / biggest_order_size\n",
    "  reorder_size_vec = np.array(reorder_lengths, dtype=np.float32) / biggest_order_size\n",
    "  order_size_data = np.repeat(order_size_vec, order_lengths, axis=0)\n",
    "  reorder_size_data = np.repeat(reorder_size_vec, order_lengths, axis=0)\n",
    "  cont_features = np.vstack([reorder_size_data, order_size_data]).T\n",
    "  \n",
    "  # Now we return a bunch of things:\n",
    "  #   orders_data is a zero-padded matrix of IDs, num instances x biggest_order_size\n",
    "  #   reorders_data is a zero-padded matrix of IDs, same size as orders_data\n",
    "  #   user_ids is a vector of IDs, length = num instances\n",
    "  #   item_ids is a vector of IDs, length = num instances\n",
    "  #   cont_features is a matrix of floats, num instances x 2\n",
    "  #   labels is a vector of 0/1 values, length = num instances\n",
    "  #   order_lengths is a vector of ints, length = batch size\n",
    "  # where \"batch size\" is the number of orders in the batch, and\n",
    "  # \"num instances\" is the total number of items in the batch\n",
    "  # (the sum of all order sizes in the batch)\n",
    "  return (orders_data, reorders_data, user_ids, item_ids, cont_features, labels, order_lengths)\n",
    "\n",
    "\n",
    "def validate(model, class_weights):\n",
    "  \n",
    "  losses = []\n",
    "  labels = []\n",
    "  outputs = []\n",
    "  split_intervals = []\n",
    "  \n",
    "  # Process validation data one batch at a time\n",
    "  steps = num_valid // batch_size\n",
    "  for chunk in np.array_split(np.arange(num_valid, dtype=np.uint16), steps):\n",
    "    \n",
    "    indices = index_valid[chunk]\n",
    "    orders_valid, reorders_valid, users_valid, items_valid, cont_features_valid, labels_valid, lengths_valid = \\\n",
    "      make_training_data(indices)\n",
    "    \n",
    "    # This is so we weight the loss differently per class, so it's\n",
    "    # comparable with training loss\n",
    "    instance_weights = class_weights[labels_valid]\n",
    "      \n",
    "    split_intervals.append(np.cumsum(lengths_valid))\n",
    "    \n",
    "    # Feed the validation data into the model, with labels, and get the\n",
    "    # value of the loss function defined by the model (i.e. cross-entropy)\n",
    "    loss = model.test_on_batch({'order_input': orders_valid,\n",
    "                                'reorder_input': reorders_valid,\n",
    "                                'item_input': items_valid,\n",
    "                                'user_input': users_valid,\n",
    "                                'cont_features_input': cont_features_valid},\n",
    "                               labels_valid, sample_weight=instance_weights)\n",
    "    losses.append(loss)\n",
    "    labels.append(labels_valid)\n",
    "\n",
    "    # Feed the validation data into the model WITHOUT labels, and get\n",
    "    # the output of the final layer of the model -- this is a vector\n",
    "    # with the same length as the number of instances (total items)\n",
    "    # in the validation set\n",
    "    output = model.predict_on_batch({'order_input': orders_valid,\n",
    "                                     'reorder_input': reorders_valid,\n",
    "                                     'item_input': items_valid,\n",
    "                                     'user_input': users_valid,\n",
    "                                     'cont_features_input': cont_features_valid})\n",
    "    outputs.append(output)\n",
    "    \n",
    "  # Flatten per-batch outputs and labels into single arrays, then\n",
    "  # re-split labels into one array per order\n",
    "  outputs_flat = np.vstack(outputs)\n",
    "  labels_flat = np.hstack(labels)\n",
    "  split_intervals_flat = np.hstack(split_intervals)\n",
    "  labels_split = np.split(labels_flat, split_intervals_flat)[:-1]\n",
    "  \n",
    "  # Convert the continuous output values (0-1 range) into 1s and 0s\n",
    "  thresholded = np.where(outputs_flat > 0.5, 1, 0)\n",
    "\n",
    "  # Split these binary predictions into individual arrays, one for\n",
    "  # each order in the validation data, so they align with the labels\n",
    "  preds_split = np.split(thresholded, split_intervals_flat)[:-1]\n",
    "  \n",
    "  # Calculate precision and recall for each order, taking care to\n",
    "  # avoid divide-by-zero errors where there are no 1s in the\n",
    "  # predictions or labels for an order\n",
    "  precision = np.zeros(num_valid)\n",
    "  recall = np.zeros(num_valid)\n",
    "  for i, (preds, labels) in enumerate(zip(preds_split, labels_split)):\n",
    "    matches = float(sum(preds.T[0] * labels))\n",
    "    if sum(preds) > 0:\n",
    "      precision[i] = matches / sum(preds)\n",
    "    if sum(labels) > 0:\n",
    "      recall[i] = matches / sum(labels)\n",
    "    \n",
    "  # Calculate f-measure for each order, and return all these metrics,\n",
    "  # averaged across all the orders in the validation set\n",
    "  f1 = np.nan_to_num((2 * precision * recall) / (precision + recall))\n",
    "  return (np.mean(losses), precision.mean(), recall.mean(), f1.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally it's time to train the model.\n",
    "\n",
    "We do this by defining a generator that Keras runs in a separate thread -- this is invoked repeatedly by Keras, and each time, it grabs the next batch of indices and yields the result of calling `make_training_data` on these.\n",
    "\n",
    "The `epochs` constant configures how many whole passes through the data we do, scoring the model's predictions on the validation set after each one. After the last epoch you can re-run the cell manually -- it will continue training from where it left off, although the epoch number reported will reset to \"1\" when you restart it. If you want to totally reset the model, rerun the model definition cell above.\n",
    "\n",
    "### Note about warnings\n",
    "\n",
    "You may see a warning about a `StopIteration` exception at the end of each epoch. It's safe to ignore this, it's an artefact of how Keras invokes the generator.\n",
    "\n",
    "You might also see a warning about invalid values in a divide, which occurs if one or more orders have both a precision _and_ recall of 0. It's also fine to ignore this, as the resulting NaNs are converted to zeros before averaging across the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "steps_per_epoch = num_rows // batch_size\n",
    "epochs = 10\n",
    "\n",
    "print(\"Batch size: %d\" % batch_size)\n",
    "print(\"Training examples: %d\" % num_rows)\n",
    "print(\"Batches per epoch: %d\" % steps_per_epoch)\n",
    "\n",
    "print(\"Initial validation: loss = %0.5f, P = %0.5f, R = %0.5f, F = %0.5f\"\n",
    "      % validate(model, class_weights))\n",
    "\n",
    "for epoch in range(epochs):\n",
    "  \n",
    "  def train_data_generator():\n",
    "    for chunk in np.array_split(index_train, steps_per_epoch):\n",
    "      orders_data, reorders_data, user_ids, item_ids, cont_features, labels, item_lengths = \\\n",
    "        make_training_data(chunk)\n",
    "      yield ({'order_input': orders_data,\n",
    "              'reorder_input': reorders_data,\n",
    "              'item_input': item_ids,\n",
    "              'user_input': user_ids,\n",
    "              'cont_features_input': cont_features},\n",
    "             {'output': labels})\n",
    "    return\n",
    "\n",
    "  # Train the model on the entire training set\n",
    "  print(\"Starting epoch %d\" % epoch)\n",
    "  print(\"Learning rate: %0.5f\" %\n",
    "        K.eval(model.optimizer.lr * (1. / (1. + model.optimizer.decay * model.optimizer.iterations))))\n",
    "  \n",
    "  model.fit_generator(\n",
    "    train_data_generator(), steps_per_epoch=steps_per_epoch, epochs=1, max_q_size=1,\n",
    "    class_weight={0: class_weights[0], 1: class_weights[1]})\n",
    "  \n",
    "  # Score it against the validation set\n",
    "  print(\"Validation: loss = %0.5f, P = %0.5f, R = %0.5f, F = %0.5f\"\n",
    "        % validate(model, class_weights))\n",
    "  \n",
    "  # Shuffle the training data (actually just the indices) for next epoch\n",
    "  print(\"Shuffling training data\")\n",
    "  np.random.shuffle(index_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "* Build _separate_ deep averaging networks over the order and reorder embeddings\n",
    "* Then concatenate their final layers with the item and continuous features\n",
    "* Then add dropout over the input items, as in the DAN paper\n",
    "\n",
    "\"Deep Unordered Composition Rivals Syntactic Methods for Text Classification\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remaining work\n",
    "\n",
    "There are a few outstanding TODOs that would make this training loop more useful on real work:\n",
    "\n",
    "* Make the generator 'greedier' by using multiple workers and a longer queue, this should speed it up\n",
    "* See if it's possible to push some of the work of `make_training_data` into TensorFlow on the GPU instead\n",
    "* Save the model to disk after every epoch\n",
    "* Terminate training early if validation F-measure stagnates\n",
    "* Reduce the learning rate manually if the training loss stagnates\n",
    "* Test on a held-out set from the end of the time period (see notes at top of file)\n",
    "\n",
    "Also, the process of experimenting with the model structure or constants (size of layers, dropout rate etc.), or the optimizer parameters, is pretty tedious to do by hand.\n",
    "\n",
    "In real life, you'd want to write a hyperparameter tuning script that automatically generates a bunch of different model variants, trains them, tests them on the same validation data, and reports the results. You can do this in parallel over multiple servers to speed up the process."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
