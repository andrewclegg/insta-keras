{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "\n",
    "This notebook defines and trains a Keras network to predict, for each order, which of the items in the user's _previous_ order will reappear in it.\n",
    "\n",
    "This is slightly different from the goal of the Kaggle contest, which is to predict which items from _any_ point in the user's past history will reappear in this order.\n",
    "\n",
    "Make sure you run the [Data Preparation](1-DataPrep.ipynb) notebook before this one.\n",
    "\n",
    "Training will be performed on a GPU automatically if your machine has one, or CPU otherwise (assuming you haven't messed with Keras's configuration).\n",
    "\n",
    "## Problem definition\n",
    "\n",
    "Conceptually, we're trying to predict a number of items for each order -- a multilabel classification problem.\n",
    "\n",
    "However, since the candidate items for each order come from a restricted set (the _n_ items in the previous order), this decomposes nicely into _n_ individual binary classification problems. This is generally an easier problem to solve, especially when _n_ is much smaller than the total number of items in the data.\n",
    "\n",
    "So, each training (or test) instance is actually an item in an order, and the label is 1 if that item also appears in the next order, and 0 otherwise.\n",
    "\n",
    "This means each order yields _n_ separate training instances, one for each item it contains.\n",
    "\n",
    "## Network structure\n",
    "\n",
    "The inputs to the network, for each training instance, are:\n",
    "\n",
    "* (1) The order that this instance came from\n",
    " * This remains the same for all the _n_ instances from this order\n",
    "* (2) The items in that order which were themselves reorders\n",
    " * This also remains the same for all instances from this order\n",
    "* (3) The user\n",
    " * This also remains the same for all instances from this order\n",
    "* (4) The item itself\n",
    " * This is the only input that differs between instances from the same order\n",
    "\n",
    "The label, for each instance, is a 1 or 0 indicating whether or not it reappears in the user's next order.\n",
    "\n",
    "All items are represented by an _embedding_, i.e. a vector of floats. These values are learnt during training, along with all the other weights of the network, and items which appear in similar contexts will result in similar embeddings.\n",
    "\n",
    "A collection of items (i.e. (1) and (2)) is represented by getting the embeddings for those items and simply adding them together, so an empty collection is just a vector of zeros.\n",
    "\n",
    "Each user is also represented by an embedding, again learnt during training.\n",
    "\n",
    "The final input to the network, then, is a concatenation of four vectors:\n",
    "\n",
    "* The order vector (sum of item embeddings in order)\n",
    "* The reorder vector (sum of reordered item embeddings in order)\n",
    "* The current item's embedding\n",
    "* The current user's embedding\n",
    "\n",
    "Or more visually:\n",
    "\n",
    "```\n",
    "|---Order-Vector-6---||--Reorder-Vector-6--||---Item-Vector-13---||---User-Vector-22---|\n",
    "|---Order-Vector-6---||--Reorder-Vector-6--||---Item-Vector-43---||---User-Vector-22---|\n",
    "|---Order-Vector-6---||--Reorder-Vector-6--||---Item-Vector-91---||---User-Vector-22---|\n",
    "...\n",
    "|---Order-Vector-9---||--Reorder-Vector-9--||---Item-Vector-10---||---User-Vector-71---|\n",
    "|---Order-Vector-9---||--Reorder-Vector-9--||---Item-Vector-13---||---User-Vector-71---|\n",
    "...\n",
    "```\n",
    "\n",
    "These vectors are fed through two consecutive fully-connected layers, each the same width, and then to a single output node which predicts the label. This is scored against the true label via [cross-entropy loss](https://jamesmccaffrey.wordpress.com/2013/11/05/why-you-should-use-cross-entropy-error-instead-of-classification-error-or-mean-squared-error-for-neural-network-classifier-training/).\n",
    "\n",
    "## Evaluation metrics\n",
    "\n",
    "Before training, 5000 randomly-selected order pairs will be set aside as validation data. After each training epoch -- i.e. each pass through the training data -- we'll evaluate the model's predictive power over the validation set, reporting [precision, recall and F1 score](https://en.wikipedia.org/wiki/Precision_and_recall) (balanced F-measure) averaged over these orders.\n",
    "\n",
    "N.B. This isn't necessarily a perfect predictor of how well the model will do in production, as the training data can potentially contain order pairs that appear chronologically later in the input data than order pairs in the validation data. This is a violation of the \"no time machines\" rule. But, it's convenient for demonstration purposes. A thorough evaluation would involve a held-out test set consisting of each user's most recent order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras as k\n",
    "import sklearn as sk\n",
    "import tensorflow as tf\n",
    "import h5py\n",
    "import os\n",
    "\n",
    "h5_dir = 'h5/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reopen saved datasets\n",
    "\n",
    "Pull all the data out of HDF5 file and back into memory. This is much faster than doing random access directly on the files, particularly if you're using a cloud server with network-attached storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datafile = h5py.File(os.path.join(h5_dir, 'training_data.h5'), 'r')\n",
    "orders_dataset = datafile['orders'][:]\n",
    "reorders_dataset = datafile['reorders'][:]\n",
    "users_dataset = datafile['users'][:]\n",
    "items_dataset = datafile['items'][:]\n",
    "labels_dataset = datafile['labels'][:]\n",
    "num_rows = datafile['num_rows'][0]\n",
    "biggest_order_size = datafile['biggest_order_size'][0]\n",
    "max_product_id = datafile['max_product_id'][0]\n",
    "max_user_id = datafile['max_user_id'][0]\n",
    "datafile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network helpers\n",
    "\n",
    "Not all Keras layers support masking (e.g. ignoring product ID 0 which is effectively 'null' in our data).\n",
    "\n",
    "So we create a new type of layer that removes masked-out values before passing the data on to the next layer. See: https://github.com/fchollet/keras/issues/2728\n",
    "\n",
    "Also we create a couple of helper functions to let us sum all the vectors coming from a previous layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "from keras.engine.topology import Layer\n",
    "\n",
    "class ZeroMaskedEntries(Layer):\n",
    "    \"\"\"\n",
    "    This layer is called after an Embedding layer.\n",
    "    It zeros out all of the masked-out embeddings.\n",
    "    It also swallows the mask without passing it on.\n",
    "    You can change this to default pass-on behavior as follows:\n",
    "\n",
    "    def compute_mask(self, x, mask=None):\n",
    "        if not self.mask_zero:\n",
    "            return None\n",
    "        else:\n",
    "            return K.not_equal(x, 0)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        self.support_mask = True\n",
    "        super(ZeroMaskedEntries, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.output_dim = input_shape[1]\n",
    "        self.repeat_dim = input_shape[2]\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        mask = K.cast(mask, 'float32')\n",
    "        mask = K.repeat(mask, self.repeat_dim)\n",
    "        mask = K.permute_dimensions(mask, (0, 2, 1))\n",
    "        return x * mask\n",
    "\n",
    "    def compute_mask(self, input_shape, input_mask=None):\n",
    "        return None\n",
    "\n",
    "def sum_layer_output_shape(input_shape):\n",
    "    shape = list(input_shape)\n",
    "    assert len(shape) == 3 \n",
    "    return (shape[0], shape[2])\n",
    "\n",
    "def sum_layer(x):\n",
    "  return K.sum(x, axis=1, keepdims=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â Network structure\n",
    "\n",
    "The following code defines the structure of the neural network, as described in the notes at the top of this file.\n",
    "\n",
    "The constants at the top define the dimensionality of the user and product embeddings, and the hidden layer sizes. The final dimensionality of the concatenated input vector is:\n",
    "\n",
    "    (product_embedding_size * 3) + user_embedding_size\n",
    "\n",
    "because we have order, reorder and current item vectors, again as described above.\n",
    "\n",
    "The hidden layers are [exponential linear units](https://arxiv.org/abs/1511.07289) with [dropout](http://jmlr.org/papers/v15/srivastava14a.html). The network is trained using the [ADAM optimizer](https://arxiv.org/abs/1412.6980).\n",
    "\n",
    "### Note\n",
    "\n",
    "When you run the following cell, no data actually gets processed.\n",
    "\n",
    "The Keras API calls here just _define_ the network structure -- think of it like a declarative DSL for describing a network.\n",
    "\n",
    "We'll actually pass data into the network, and evaluate its output, later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "order_input (InputLayer)         (None, 145)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "reorder_input (InputLayer)       (None, 145)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "item_input (InputLayer)          (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "product_embedding (Embedding)    multiple              2484450     order_input[0][0]                \n",
      "                                                                   reorder_input[0][0]              \n",
      "                                                                   item_input[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "user_input (InputLayer)          (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "zero_masked_entries_7 (ZeroMaske (None, 145, 50)       0           product_embedding[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "zero_masked_entries_8 (ZeroMaske (None, 145, 50)       0           product_embedding[1][0]          \n",
      "____________________________________________________________________________________________________\n",
      "flattened_item (Lambda)          (None, 1, 50)         0           product_embedding[2][0]          \n",
      "____________________________________________________________________________________________________\n",
      "user_embedding (Embedding)       (None, 1, 50)         10310500    user_input[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "order_vector (Lambda)            (None, 50)            0           zero_masked_entries_7[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "reorder_vector (Lambda)          (None, 50)            0           zero_masked_entries_8[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "item_vector (Flatten)            (None, 50)            0           flattened_item[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "user_vector (Flatten)            (None, 50)            0           user_embedding[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "full_input_vector (Concatenate)  (None, 200)           0           order_vector[0][0]               \n",
      "                                                                   reorder_vector[0][0]             \n",
      "                                                                   item_vector[0][0]                \n",
      "                                                                   user_vector[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "hidden_1 (Dense)                 (None, 500)           100500      full_input_vector[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 500)           0           hidden_1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "hidden_2 (Dense)                 (None, 500)           250500      dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 500)           0           hidden_2[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "hidden_3 (Dense)                 (None, 250)           125250      dropout_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "output (Dense)                   (None, 1)             251         hidden_3[0][0]                   \n",
      "====================================================================================================\n",
      "Total params: 13,271,451\n",
      "Trainable params: 13,271,451\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Layer size constants\n",
    "user_embedding_size = 50\n",
    "product_embedding_size = 50\n",
    "hidden_1_size = 500\n",
    "hidden_2_size = 500\n",
    "hidden_3_size = 250\n",
    "\n",
    "# Activation function for the hidden layers\n",
    "activation = 'elu'\n",
    "\n",
    "# Dropout rate for the hidden layers\n",
    "dropout = 0.33\n",
    "\n",
    "# Initial learning rate\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Input layers for the four datasets\n",
    "\n",
    "user_input = k.layers.Input(shape=(1,), name='user_input')\n",
    "\n",
    "order_input = k.layers.Input(shape=(biggest_order_size,), name='order_input')\n",
    "\n",
    "reorder_input = k.layers.Input(shape=(biggest_order_size,), name='reorder_input')\n",
    "\n",
    "item_input = k.layers.Input(shape=(1,), name='item_input')\n",
    "\n",
    "# Set up the product embeddings -- this gets reused to map the order, reorder and\n",
    "# individual item IDs into embeddings\n",
    "# Think of it as a lookup table mapping IDs to vectors of floats\n",
    "\n",
    "product_embedding = k.layers.Embedding(\n",
    "  input_dim=max_product_id + 1, output_dim=product_embedding_size,\n",
    "  mask_zero=True, name='product_embedding')\n",
    "\n",
    "# Set up a separate user embedding table\n",
    "\n",
    "user_embedding = k.layers.Embedding(\n",
    "  input_dim=max_user_id + 1, output_dim=user_embedding_size, name='user_embedding')\n",
    "\n",
    "# Sum up the embeddings in the order and reorder sets into order and reorder \n",
    "\n",
    "order_embedding = ZeroMaskedEntries()(product_embedding(order_input))\n",
    "\n",
    "order_vector = k.layers.Lambda(sum_layer, sum_layer_output_shape, name='order_vector')(\n",
    "  order_embedding)\n",
    "\n",
    "reorder_embedding = ZeroMaskedEntries()(product_embedding(reorder_input))\n",
    "\n",
    "reorder_vector = k.layers.Lambda(sum_layer, sum_layer_output_shape, name='reorder_vector')(\n",
    "  reorder_embedding)\n",
    "\n",
    "# Flatten the single item embedding into a vector\n",
    "# Hack: Flatten also doesn't support masks, but we don't need to do anything special\n",
    "# The item embeddings will never be masked out, there's always exactly one in each\n",
    "\n",
    "flattened_item = k.layers.Lambda(\n",
    "  lambda x: x, output_shape=lambda s:s, name='flattened_item')\n",
    "\n",
    "item_vector = k.layers.Flatten(name='item_vector')(\n",
    "  flattened_item(product_embedding(item_input)))\n",
    "\n",
    "# Flatten the single user embedding into a vector\n",
    "\n",
    "user_vector = k.layers.Flatten(name='user_vector')(\n",
    "  user_embedding(user_input))\n",
    "\n",
    "full_input_vector = k.layers.concatenate(\n",
    "  [order_vector, reorder_vector, item_vector, user_vector], name=\"full_input_vector\")\n",
    "\n",
    "# Define the hidden layers\n",
    "\n",
    "hidden_1 = k.layers.Dense(hidden_1_size, activation=activation, name='hidden_1')(full_input_vector)\n",
    "dropout_1 = k.layers.Dropout(dropout, name='dropout_1')(hidden_1)\n",
    "\n",
    "hidden_2 = k.layers.Dense(hidden_2_size, activation=activation, name='hidden_2')(dropout_1)\n",
    "dropout_2 = k.layers.Dropout(dropout, name='dropout_2')(hidden_2)\n",
    "\n",
    "hidden_3 = k.layers.Dense(hidden_3_size, activation=activation, name='hidden_3')(dropout_2)\n",
    "dropout_3 = k.layers.Dropout(dropout, name='dropout_3')(hidden_3)\n",
    "\n",
    "output = k.layers.Dense(1, activation='sigmoid', name='output')(hidden_3)\n",
    "\n",
    "# Compile the model\n",
    "\n",
    "model = k.models.Model(\n",
    "  inputs=[order_input, reorder_input, user_input, item_input], outputs=output)\n",
    "\n",
    "model.compile(\n",
    "  optimizer=tf.train.AdamOptimizer(learning_rate=learning_rate), loss='binary_crossentropy')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO model graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "The following sections set up the training process.\n",
    "\n",
    "We read the data in batches of 1000 order pairs at a time, although each of these gets expanded into one training instance per item in the order, as described earlier.\n",
    "\n",
    "The data is shuffled before training starts, and again at the end of each training epoch, because data with non-random ordering can cause batch-based optimization algorithms to behave in unexpected ways and perform badly.\n",
    "\n",
    "Note that shuffling the data is done indirectly, via a numeric index into the input datasets, rather than by trying to shuffle them in-place but in a synchronized manner. This makes the code much simpler, and is faster too.\n",
    "\n",
    "We set aside 5000 order pairs as the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "num_valid = 5000\n",
    "\n",
    "index = np.arange(num_rows, dtype=np.uint32)\n",
    "np.random.shuffle(index)\n",
    "index_valid = index[:num_valid]\n",
    "index_train = index[num_valid:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define a couple of helper functions.\n",
    "\n",
    "`make_training_data` is called once per batch, on a list of indices into the training data. It retrieves the records corresponding to those indices from the underlying datasets, performs some minor adjustments and returns them as Numpy arrays.\n",
    "\n",
    "`validate` is called once per epoch, i.e. after one whole run through the training data. It calculates some metrics on the validation data and returns them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_training_data(indices):\n",
    "  \n",
    "  # Get labels for this batch's instances, and stack them into a single vector\n",
    "  labels_separate = labels_dataset[indices]\n",
    "  labels = np.hstack(labels_separate)\n",
    "  \n",
    "  # Get the arrays of ordered item IDs and reordered item IDs for each order\n",
    "  # (these are already zero-padded to the same length)\n",
    "  orders_separate = orders_dataset[indices]\n",
    "  reorders_separate = reorders_dataset[indices]\n",
    "  \n",
    "  # From the length of each individual label array, we know how many items\n",
    "  # are in each of the orders\n",
    "  order_lengths = [len(items) for items in labels_separate]\n",
    "\n",
    "  # Repeat each order array and each reorder array that many times\n",
    "  orders_data = np.repeat(orders_separate, order_lengths, axis=0)\n",
    "  reorders_data = np.repeat(reorders_separate, order_lengths, axis=0)\n",
    "  \n",
    "  # Also repeat each user ID the appropriate number of times\n",
    "  user_ids = np.repeat(users_dataset[indices], order_lengths)\n",
    "  \n",
    "  # We don't need to repeat the item IDs, each training instance corresponds\n",
    "  # to a single item, to we just concatenate them into a single array\n",
    "  item_ids = np.hstack(items_dataset[indices])\n",
    "  \n",
    "  # Now we return a bunch of things:\n",
    "  #   orders_data is a zero-padded matrix of IDs, num instances x biggest_order_size\n",
    "  #   reorders_data is a zero-padded matrix of IDs, same size as orders_data\n",
    "  #   user_ids is a vector of IDs, length = num instances\n",
    "  #   item_ids is a vector of IDs, length = num instances\n",
    "  #   labels is a vector of 0/1 values, length = num instances\n",
    "  #   order_lengths is a vector of ints, length = batch size\n",
    "  # where \"batch size\" is the number of orders in the batch, and\n",
    "  # \"num instances\" is the total number of items in the batch\n",
    "  # (the sum of all order sizes in the batch)\n",
    "  return (orders_data, reorders_data, user_ids, item_ids, labels, order_lengths)\n",
    "\n",
    "def validate(model):\n",
    "  \n",
    "  # Feed the validation data into the model, with labels, and get the\n",
    "  # value of the loss function defined by the model (i.e. cross-entropy)\n",
    "  loss = model.test_on_batch({'order_input': valid_order_data,\n",
    "                              'reorder_input': valid_reorder_data,\n",
    "                              'item_input': valid_item_ids,\n",
    "                              'user_input': valid_user_ids}, valid_labels)\n",
    "  \n",
    "  # Feed the validation data into the model WITHOUT labels, and get\n",
    "  # the output of the final layer of the model\n",
    "  output = model.predict_on_batch({'order_input': valid_order_data,\n",
    "                                   'reorder_input': valid_reorder_data,\n",
    "                                   'item_input': valid_item_ids,\n",
    "                                   'user_input': valid_user_ids})\n",
    "  \n",
    "  preds = np.where(output > 0.5, 1, 0)\n",
    "  preds_split = np.split(preds, valid_split_intervals)[:-1]\n",
    "  precision = np.zeros(num_valid)\n",
    "  recall = np.zeros(num_valid)\n",
    "  for i, (preds, labels) in enumerate(zip(preds_split, valid_labels_split)):\n",
    "    matches = float(sum(preds.T[0] * labels))\n",
    "    if sum(preds) > 0:\n",
    "      precision[i] = matches / sum(preds)\n",
    "    if sum(labels) > 0:\n",
    "      recall[i] = matches / sum(labels)\n",
    "  f1 = np.nan_to_num((2 * precision * recall) / (precision + recall))\n",
    "  return (loss, precision.mean(), recall.mean(), f1.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid_order_data, valid_reorder_data, valid_user_ids, valid_item_ids, valid_labels, valid_lengths = make_training_data(index_valid)\n",
    "valid_split_intervals = np.cumsum(valid_lengths)\n",
    "valid_labels_split = np.split(valid_labels, valid_split_intervals)[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 1000\n",
      "Training examples: 2933665\n",
      "Batches per epoch: 2933\n",
      "Starting epoch 0\n",
      "Epoch 1/1\n",
      "2932/2933 [============================>.] - ETA: 0s - loss: 0.6191"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-5:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/andrew/anaconda2/lib/python2.7/threading.py\", line 801, in __bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/andrew/anaconda2/lib/python2.7/threading.py\", line 754, in run\n",
      "    self.__target(*self.__args, **self.__kwargs)\n",
      "  File \"/home/andrew/anaconda2/lib/python2.7/site-packages/keras/engine/training.py\", line 612, in data_generator_task\n",
      "    generator_output = next(self._generator)\n",
      "StopIteration\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2933/2933 [==============================] - 1402s - loss: 0.6191  \n",
      "Validation: loss = 0.60755, P = 0.34115, R = 0.48297, F = 0.36742\n",
      "Shuffling training data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrew/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:34: RuntimeWarning: invalid value encountered in divide\n"
     ]
    }
   ],
   "source": [
    "steps_per_epoch = num_rows // batch_size\n",
    "epochs = 1\n",
    "\n",
    "print(\"Batch size: %d\" % batch_size)\n",
    "print(\"Training examples: %d\" % num_rows)\n",
    "print(\"Batches per epoch: %d\" % steps_per_epoch)\n",
    "\n",
    "def train_data_generator():\n",
    "  for chunk in np.array_split(index_train, steps_per_epoch):\n",
    "    orders_data, reorders_data, user_ids, item_ids, labels, item_lengths = make_training_data(chunk)\n",
    "    yield ({'order_input': orders_data,\n",
    "            'reorder_input': reorders_data,\n",
    "            'item_input': item_ids,\n",
    "            'user_input': user_ids},\n",
    "           {'output': labels})\n",
    "  return\n",
    "\n",
    "# TODO early stopping, checkpointing, learning rate reduction\n",
    "\n",
    "for epoch in range(epochs):\n",
    "  \n",
    "  print(\"Starting epoch %d\" % epoch)\n",
    "  \n",
    "  labels_sample = np.hstack(labels_dataset[np.sort(index_train[0:num_valid]).tolist()])\n",
    "  class_weights = sk.utils.class_weight.compute_class_weight('balanced', [0, 1], labels_sample)\n",
    "  del labels_sample\n",
    "  \n",
    "  model.fit_generator(train_data_generator(), steps_per_epoch=steps_per_epoch, epochs=1, max_q_size=1,\n",
    "                      class_weight={0: class_weights[0], 1: class_weights[1]})\n",
    "  \n",
    "  print(\"Validation: loss = %0.5f, P = %0.5f, R = %0.5f, F = %0.5f\" % validate(model))\n",
    "  print(\"Shuffling training data\")\n",
    "  np.random.shuffle(index_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "50/50, 500, 500, elu/0.33, adam 0.01:\n",
    "\n",
    "0: loss = 0.60755, P = 0.34115, R = 0.48297, F = 0.36742\n",
    "\n",
    "50/50, 200, 200, elu/0.33, adam 0.01:\n",
    "\n",
    "0: loss = 0.57991, P = 0.34420, R = 0.44656, F = 0.35717\n",
    "\n",
    "50/50, 200, 200, relu/0.33, adam 0.01:\n",
    "\n",
    "0: loss = 0.58145, P = 0.33895, R = 0.44538, F = 0.35334"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
