{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras as k\n",
    "import keras.backend as K\n",
    "from keras.engine.topology import Layer\n",
    "import sklearn as sk\n",
    "import tensorflow as tf\n",
    "import h5py\n",
    "import os\n",
    "from itertools import izip\n",
    "\n",
    "h5_dir = 'h5/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reopen saved datasets\n",
    "\n",
    "Pull all the data out of HDF5 file and back into memory. This is much faster than doing random access directly on the files, particularly if you're using a cloud server with network-attached storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datafile = h5py.File(os.path.join(h5_dir, 'training_data.h5'), 'r')\n",
    "orders_dataset = datafile['orders'][:]\n",
    "reorders_dataset = datafile['reorders'][:]\n",
    "users_dataset = datafile['users'][:]\n",
    "labels_dataset = datafile['labels'][:]\n",
    "num_rows = datafile['num_rows'][0]\n",
    "biggest_order_size = datafile['biggest_order_size'][0]\n",
    "max_product_id = datafile['max_product_id'][0]\n",
    "max_user_id = datafile['max_user_id'][0]\n",
    "datafile.close()\n",
    "del datafile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network helpers\n",
    "\n",
    "From: https://github.com/fchollet/keras/issues/2728"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ZeroMaskedEntries(k.engine.topology.Layer):\n",
    "    \"\"\"\n",
    "    This layer is called after an Embedding layer.\n",
    "    It zeros out all of the masked-out embeddings.\n",
    "    It also swallows the mask without passing it on.\n",
    "    You can change this to default pass-on behavior as follows:\n",
    "\n",
    "    def compute_mask(self, x, mask=None):\n",
    "        if not self.mask_zero:\n",
    "            return None\n",
    "        else:\n",
    "            return K.not_equal(x, 0)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        self.support_mask = True\n",
    "        super(ZeroMaskedEntries, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.output_dim = input_shape[1]\n",
    "        self.repeat_dim = input_shape[2]\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        mask = K.cast(mask, 'float32')\n",
    "        mask = K.repeat(mask, self.repeat_dim)\n",
    "        mask = K.permute_dimensions(mask, (0, 2, 1))\n",
    "        return x * mask\n",
    "\n",
    "    def compute_mask(self, input_shape, input_mask=None):\n",
    "        return None\n",
    "\n",
    "\n",
    "def mask_aware_mean(x):\n",
    "    # recreate the masks - all zero rows have been masked\n",
    "    mask = K.not_equal(K.sum(K.abs(x), axis=2, keepdims=True), 0)\n",
    "\n",
    "    # number of that rows are not all zeros\n",
    "    n = K.sum(K.cast(mask, 'float32'), axis=1, keepdims=False)\n",
    "    \n",
    "    # compute mask-aware mean of x, or all zeroes if no rows present\n",
    "    x_mean = K.sum(x, axis=1, keepdims=False) / n\n",
    "    x_mean = tf.where(tf.is_nan(x_mean), tf.zeros_like(x_mean), x_mean)\n",
    "    x_mean = tf.verify_tensor_all_finite(x_mean, 'fuck')\n",
    "\n",
    "    return x_mean\n",
    "\n",
    "\n",
    "def mask_aware_mean_output_shape(input_shape):\n",
    "    shape = list(input_shape)\n",
    "    assert len(shape) == 3 \n",
    "    return (shape[0], shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def soft_f1(target, output):\n",
    "  # 'Thresholding' with very steep logistic function\n",
    "  #thresholded = 1.0 / (1.0 + k.backend.exp(-10000.0 * output))\n",
    "  #thresholded = k.backend.sigmoid(output)\n",
    "  epsilon = 1e-12\n",
    "  matches = target * output\n",
    "  all_pos = k.backend.sum(output, axis=1)\n",
    "  all_true = k.backend.sum(target, axis=1)\n",
    "  true_pos = k.backend.sum(matches, axis=1)\n",
    "  precision = true_pos / (all_pos + epsilon)\n",
    "  recall = true_pos / (all_true + epsilon)\n",
    "  f1 = (2 * precision * recall) / (precision + recall + epsilon)\n",
    "  mean_f1 = 1.0 - k.backend.mean(f1)\n",
    "  return mean_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â Network structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "order_input (InputLayer)         (None, 145)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "reorder_input (InputLayer)       (None, 145)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "product_embedding_in (Embedding) (None, 145, 50)       2484450     order_input[0][0]                \n",
      "                                                                   reorder_input[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "lambda_27 (Lambda)               (None, 50)            0           product_embedding_in[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "order_sizes_input (InputLayer)   (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "lambda_28 (Lambda)               (None, 50)            0           product_embedding_in[1][0]       \n",
      "____________________________________________________________________________________________________\n",
      "reorder_sizes_input (InputLayer) (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "order_vector_ext (Concatenate)   (None, 51)            0           lambda_27[0][0]                  \n",
      "                                                                   order_sizes_input[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "reorder_vector_ext (Concatenate) (None, 51)            0           lambda_28[0][0]                  \n",
      "                                                                   reorder_sizes_input[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "order_hidden_1 (Dense)           (None, 50)            2550        order_vector_ext[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "reorder_hidden_1 (Dense)         (None, 50)            2550        reorder_vector_ext[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_66 (BatchNor (None, 50)            150         order_hidden_1[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_69 (BatchNor (None, 50)            150         reorder_hidden_1[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "activation_87 (Activation)       (None, 50)            0           batch_normalization_66[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_90 (Activation)       (None, 50)            0           batch_normalization_69[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "dropout_87 (Dropout)             (None, 50)            0           activation_87[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dropout_90 (Dropout)             (None, 50)            0           activation_90[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "order_hidden_2 (Dense)           (None, 50)            2500        dropout_87[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "reorder_hidden_2 (Dense)         (None, 50)            2500        dropout_90[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_67 (BatchNor (None, 50)            150         order_hidden_2[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_70 (BatchNor (None, 50)            150         reorder_hidden_2[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "activation_88 (Activation)       (None, 50)            0           batch_normalization_67[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_91 (Activation)       (None, 50)            0           batch_normalization_70[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "dropout_88 (Dropout)             (None, 50)            0           activation_88[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dropout_91 (Dropout)             (None, 50)            0           activation_91[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "order_hidden_3 (Dense)           (None, 50)            2500        dropout_88[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "reorder_hidden_3 (Dense)         (None, 50)            2500        dropout_91[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_68 (BatchNor (None, 50)            150         order_hidden_3[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_71 (BatchNor (None, 50)            150         reorder_hidden_3[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "user_input (InputLayer)          (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "activation_89 (Activation)       (None, 50)            0           batch_normalization_68[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_92 (Activation)       (None, 50)            0           batch_normalization_71[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "user_embedding (Embedding)       (None, 1, 50)         10310500    user_input[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dropout_89 (Dropout)             (None, 50)            0           activation_89[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dropout_92 (Dropout)             (None, 50)            0           activation_92[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "user_vector (Flatten)            (None, 50)            0           user_embedding[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "full_input_vector (Concatenate)  (None, 150)           0           dropout_89[0][0]                 \n",
      "                                                                   dropout_92[0][0]                 \n",
      "                                                                   user_vector[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "hidden (Dense)                   (None, 150)           22500       full_input_vector[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_72 (BatchNor (None, 150)           600         hidden[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "activation_93 (Activation)       (None, 150)           0           batch_normalization_72[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "dropout_93 (Dropout)             (None, 150)           0           activation_93[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "output (Dense)                   (None, 49689)         7503039     dropout_93[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 20,337,089\n",
      "Trainable params: 20,336,189\n",
      "Non-trainable params: 900\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Layer size constants\n",
    "user_embedding_size = 50\n",
    "product_embedding_size = 50\n",
    "\n",
    "# Activation function for the hidden layers\n",
    "activation = 'relu'\n",
    "\n",
    "# Dropout rate for the hidden layers\n",
    "dropout = 0.1\n",
    "\n",
    "# Initial learning rate etc.\n",
    "learning_rate = 0.01\n",
    "#decay_rate = 1e-5\n",
    "\n",
    "def bn_active_dropout(input, scale=False):\n",
    "  return k.layers.Dropout(dropout)(\n",
    "    k.layers.Activation(activation)(\n",
    "      k.layers.normalization.BatchNormalization(scale=scale)(input)))\n",
    "\n",
    "# Input layers for the datasets, and the continuous features\n",
    "\n",
    "user_input = k.layers.Input(shape=(1,), name='user_input')\n",
    "\n",
    "order_input = k.layers.Input(shape=(biggest_order_size,), name='order_input')\n",
    "\n",
    "reorder_input = k.layers.Input(shape=(biggest_order_size,), name='reorder_input')\n",
    "\n",
    "order_sizes_input = k.layers.Input(shape=(1,), name='order_sizes_input')\n",
    "\n",
    "reorder_sizes_input = k.layers.Input(shape=(1,), name='reorder_sizes_input')\n",
    "\n",
    "# Set up the embeddings tables -- these map the order, reorder, and user IDs\n",
    "# into embeddings\n",
    "# Think of them as lookup tables mapping IDs to vectors of floats\n",
    "\n",
    "product_embedding_in = k.layers.Embedding(\n",
    "  input_dim=max_product_id + 1, output_dim=product_embedding_size,\n",
    "  name='product_embedding_in')\n",
    "\n",
    "# Set up a separate user embedding table\n",
    "\n",
    "user_embedding = k.layers.Embedding(\n",
    "  input_dim=max_user_id + 1, output_dim=user_embedding_size, name='user_embedding')\n",
    "\n",
    "# Deep averaging networks for the order and reorder sets\n",
    "# We add order size and reorder size features to averaged vectors, before passing them thru the DANs\n",
    "\n",
    "order_embedding = product_embedding_in(order_input)\n",
    "\n",
    "order_vector = k.layers.Lambda(\n",
    "  mask_aware_mean, mask_aware_mean_output_shape)(order_embedding)\n",
    "\n",
    "order_vector_ext = k.layers.concatenate(\n",
    "  [order_vector, order_sizes_input], name='order_vector_ext')\n",
    "\n",
    "order_hidden_1 = bn_active_dropout(k.layers.Dense(\n",
    "  product_embedding_size, name='order_hidden_1', use_bias=False)(order_vector_ext))\n",
    "\n",
    "order_hidden_2 = bn_active_dropout(k.layers.Dense(\n",
    "  product_embedding_size, name='order_hidden_2', use_bias=False)(order_hidden_1))\n",
    "\n",
    "order_hidden_3 = bn_active_dropout(k.layers.Dense(\n",
    "  product_embedding_size, name='order_hidden_3', use_bias=False)(order_hidden_2))\n",
    "\n",
    "reorder_embedding = product_embedding_in(reorder_input)\n",
    "\n",
    "reorder_vector = k.layers.Lambda(\n",
    "  mask_aware_mean, mask_aware_mean_output_shape)(reorder_embedding)\n",
    "\n",
    "reorder_vector_ext = k.layers.concatenate(\n",
    "  [reorder_vector, reorder_sizes_input], name='reorder_vector_ext')\n",
    "\n",
    "reorder_hidden_1 = bn_active_dropout(k.layers.Dense(\n",
    "  product_embedding_size, name='reorder_hidden_1', use_bias=False)(reorder_vector_ext))\n",
    "\n",
    "reorder_hidden_2 = bn_active_dropout(k.layers.Dense(\n",
    "  product_embedding_size, name='reorder_hidden_2', use_bias=False)(reorder_hidden_1))\n",
    "\n",
    "reorder_hidden_3 = bn_active_dropout(k.layers.Dense(\n",
    "  product_embedding_size, name='reorder_hidden_3', use_bias=False)(reorder_hidden_2))\n",
    "\n",
    "# Flatten the single user embedding into a vector\n",
    "\n",
    "user_vector = k.layers.Flatten(name='user_vector')(\n",
    "  user_embedding(user_input))\n",
    "\n",
    "# Concatenate embeddings and n-hot item IDs, and output\n",
    "\n",
    "full_input_vector = k.layers.concatenate(\n",
    "  [order_hidden_3, reorder_hidden_3, user_vector],\n",
    "  name=\"full_input_vector\")\n",
    "\n",
    "# Define the hidden layer\n",
    "\n",
    "hidden = bn_active_dropout(k.layers.Dense(\n",
    "  user_embedding_size + 2 * product_embedding_size, name='hidden',\n",
    "  use_bias=False)(full_input_vector), scale=True)\n",
    "\n",
    "output = k.layers.Dense(\n",
    "  max_product_id + 1, activation='sigmoid', name='output')(hidden)\n",
    "\n",
    "# Compile the model\n",
    "\n",
    "model = k.models.Model(\n",
    "  inputs=[order_input, reorder_input, user_input, order_sizes_input, reorder_sizes_input],\n",
    "  outputs=output)\n",
    "\n",
    "model.compile(\n",
    "  optimizer=k.optimizers.Nadam(lr=learning_rate), loss=soft_f1)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "num_valid = 5000\n",
    "assert num_valid % batch_size == 0\n",
    "\n",
    "index = np.arange(num_rows, dtype=np.uint32)\n",
    "np.random.shuffle(index)\n",
    "index_valid = index[:num_valid]\n",
    "index_train = index[num_valid:]\n",
    "\n",
    "labels_train =  np.hstack(labels_dataset[index_train])\n",
    "class_weights = sk.utils.class_weight.compute_class_weight(\n",
    "  'balanced', np.unique(labels_train), labels_train)\n",
    "del labels_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define a couple of helper functions.\n",
    "\n",
    "`make_training_data` is called once per batch, on a list of indices into the training data. It retrieves the records corresponding to those indices from the underlying datasets, performs some minor adjustments and returns them as Numpy arrays.\n",
    "\n",
    "`validate` is called once per epoch, i.e. after one whole run through the training data. It calculates some metrics on the validation data -- batch by batch as if it was training data -- and returns them.\n",
    "\n",
    "\n",
    "`validate` is a bit inefficient as it regenerates the input arrays (via `make_training_data`) every time it's called, even though these don't change and could be cached. This would be easy to implement at the cost of higher memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def n_hot(item_ids):\n",
    "  output = np.zeros(max_product_id + 1, dtype=np.uint8)\n",
    "  output[item_ids] = 1\n",
    "  return output\n",
    "\n",
    "\n",
    "def make_training_data(indices):\n",
    "  \n",
    "  # Get labels for this batch's instances, and n-hot encode them\n",
    "  labels_separate = labels_dataset[indices]\n",
    "  labels_n_hot = [n_hot(order) for order in labels_separate]\n",
    "  \n",
    "  # Get the arrays of ordered item IDs and reordered item IDs for each order\n",
    "  # (these are already zero-padded to the same length)\n",
    "  orders_separate = orders_dataset[indices]\n",
    "  reorders_separate = reorders_dataset[indices]\n",
    "  \n",
    "  # Get user IDs\n",
    "  user_ids = users_dataset[indices]\n",
    "  \n",
    "  # Stack all the inputs and labels\n",
    "  orders_data = np.vstack(orders_separate)\n",
    "  reorders_data = np.vstack(orders_separate)\n",
    "  labels_data = np.vstack(labels_n_hot)\n",
    "  \n",
    "  # Calculate continuous features: currently just order size and reorder size\n",
    "  # (both normalized by biggest_order_size)\n",
    "  order_sizes = np.hstack(np.count_nonzero(order) for order in orders_separate)\n",
    "  reorder_sizes = np.hstack(np.count_nonzero(order) for order in reorders_separate)\n",
    "\n",
    "  # Now we return a bunch of things:\n",
    "  #   orders_data is a zero-padded matrix of IDs, batch size x biggest_order_size\n",
    "  #   reorders_data is a zero-padded matrix of IDs, same size as orders_data\n",
    "  #   user_ids is a vector of IDs, length = batch size\n",
    "  #   order_sizes is a vector of ints, length = batch size\n",
    "  #   reorder_sizes is a vector of ints, length = batch size\n",
    "  #   labels is a binary matrix, batch size x biggest_order_size\n",
    "  return (orders_data, reorders_data, user_ids, order_sizes, reorder_sizes, labels_data)\n",
    "\n",
    "\n",
    "def validate(model):\n",
    "  \n",
    "  losses = []\n",
    "  precisions = []\n",
    "  recalls = []\n",
    "  f_measures = []\n",
    "  \n",
    "  # Process validation data one batch at a time\n",
    "  steps = num_valid // batch_size\n",
    "  for chunk in np.array_split(np.arange(num_valid, dtype=np.uint16), steps):\n",
    "    \n",
    "    indices = index_valid[chunk]\n",
    "\n",
    "    orders_valid, reorders_valid, users_valid, order_sizes_valid, reorder_sizes_valid, labels_valid = \\\n",
    "      make_training_data(indices)\n",
    "\n",
    "    # Loss isn't really comparable to training loss as it doesn't use class weights\n",
    "    loss = model.test_on_batch({'order_input': orders_valid,\n",
    "                                'reorder_input': reorders_valid,\n",
    "                                'user_input': users_valid,\n",
    "                                'order_sizes_input': order_sizes_valid,\n",
    "                                'reorder_sizes_input': reorder_sizes_valid},\n",
    "                               labels_valid)\n",
    "    losses.append(loss)\n",
    "\n",
    "    # Predictions, for calculating P/R/F\n",
    "    output = model.predict_on_batch({'order_input': orders_valid,\n",
    "                                     'reorder_input': reorders_valid,\n",
    "                                     'user_input': users_valid,\n",
    "                                     'order_sizes_input': order_sizes_valid,\n",
    "                                     'reorder_sizes_input': reorder_sizes_valid})\n",
    "\n",
    "    # Arrays of predictions and matches, batch size x num categories\n",
    "    preds = np.where(output > 0.5, 1, 0)\n",
    "    matches = preds * labels_valid\n",
    "\n",
    "    epsilon = 1e-12\n",
    "    precision = matches.sum(axis=1) / (preds.sum(axis=1) + epsilon)\n",
    "    recall = matches.sum(axis=1) / (labels_valid.sum(axis=1) + epsilon)\n",
    "    f1 = (2 * precision * recall) / (precision + recall + epsilon)\n",
    "    \n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    f_measures.append(f1)\n",
    "  \n",
    "  return (np.array(losses).mean(), np.hstack(precisions).mean(), np.hstack(recalls).mean(), np.hstack(f_measures).mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally it's time to train the model.\n",
    "\n",
    "We do this by defining a generator that Keras runs in a separate thread -- this is invoked repeatedly by Keras, and each time, it grabs the next batch of indices and yields the result of calling `make_training_data` on these.\n",
    "\n",
    "The `epochs` constant configures how many whole passes through the data we do, scoring the model's predictions on the validation set after each one. After the last epoch you can re-run the cell manually -- it will continue training from where it left off, although the epoch number reported will reset to \"1\" when you restart it. If you want to totally reset the model, rerun the model definition cell above.\n",
    "\n",
    "### Note about warnings\n",
    "\n",
    "You may see a warning about a `StopIteration` exception at the end of each epoch. It's safe to ignore this, it's an artefact of how Keras invokes the generator.\n",
    "\n",
    "You might also see a warning about invalid values in a divide, which occurs if one or more orders have both a precision _and_ recall of 0. It's also fine to ignore this, as the resulting NaNs are converted to zeros before averaging across the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 1000\n",
      "Training examples: 2933665\n",
      "Batches per epoch: 2933\n",
      "Initial validation: loss = 0.99959, P = 0.00021, R = 0.50561, F = 0.00041\n",
      "Starting epoch 0\n",
      "Epoch 1/1\n",
      "2932/2933 [============================>.] - ETA: 0s - loss: 0.9247"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-21:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/andrew/anaconda2/lib/python2.7/threading.py\", line 801, in __bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/andrew/anaconda2/lib/python2.7/threading.py\", line 754, in run\n",
      "    self.__target(*self.__args, **self.__kwargs)\n",
      "  File \"/home/andrew/anaconda2/lib/python2.7/site-packages/keras/engine/training.py\", line 612, in data_generator_task\n",
      "    generator_output = next(self._generator)\n",
      "StopIteration\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2933/2933 [==============================] - 1127s - loss: 0.9247  \n",
      "Validation: loss = 0.92173, P = 0.09445, R = 0.08427, F = 0.07825\n",
      "Shuffling training data\n",
      "Starting epoch 1\n",
      "Epoch 1/1\n",
      "2932/2933 [============================>.] - ETA: 0s - loss: 0.9111"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-22:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/andrew/anaconda2/lib/python2.7/threading.py\", line 801, in __bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/andrew/anaconda2/lib/python2.7/threading.py\", line 754, in run\n",
      "    self.__target(*self.__args, **self.__kwargs)\n",
      "  File \"/home/andrew/anaconda2/lib/python2.7/site-packages/keras/engine/training.py\", line 612, in data_generator_task\n",
      "    generator_output = next(self._generator)\n",
      "StopIteration\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2933/2933 [==============================] - 1126s - loss: 0.9111  \n",
      "Validation: loss = 0.91012, P = 0.11724, R = 0.08951, F = 0.08985\n",
      "Shuffling training data\n",
      "Starting epoch 2\n",
      "Epoch 1/1\n",
      "2932/2933 [============================>.] - ETA: 0s - loss: 0.9056"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-23:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/andrew/anaconda2/lib/python2.7/threading.py\", line 801, in __bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/andrew/anaconda2/lib/python2.7/threading.py\", line 754, in run\n",
      "    self.__target(*self.__args, **self.__kwargs)\n",
      "  File \"/home/andrew/anaconda2/lib/python2.7/site-packages/keras/engine/training.py\", line 612, in data_generator_task\n",
      "    generator_output = next(self._generator)\n",
      "StopIteration\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2933/2933 [==============================] - 1136s - loss: 0.9056  \n",
      "Validation: loss = 0.91131, P = 0.10955, R = 0.09203, F = 0.08877\n",
      "Shuffling training data\n",
      "Starting epoch 3\n",
      "Epoch 1/1\n",
      "2932/2933 [============================>.] - ETA: 0s - loss: 0.9024"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-24:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/andrew/anaconda2/lib/python2.7/threading.py\", line 801, in __bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/andrew/anaconda2/lib/python2.7/threading.py\", line 754, in run\n",
      "    self.__target(*self.__args, **self.__kwargs)\n",
      "  File \"/home/andrew/anaconda2/lib/python2.7/site-packages/keras/engine/training.py\", line 612, in data_generator_task\n",
      "    generator_output = next(self._generator)\n",
      "StopIteration\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2933/2933 [==============================] - 1135s - loss: 0.9024  \n",
      "Validation: loss = 0.90729, P = 0.12247, R = 0.09199, F = 0.09271\n",
      "Shuffling training data\n",
      "Starting epoch 4\n",
      "Epoch 1/1\n",
      "2932/2933 [============================>.] - ETA: 0s - loss: 0.8994"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-25:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/andrew/anaconda2/lib/python2.7/threading.py\", line 801, in __bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/andrew/anaconda2/lib/python2.7/threading.py\", line 754, in run\n",
      "    self.__target(*self.__args, **self.__kwargs)\n",
      "  File \"/home/andrew/anaconda2/lib/python2.7/site-packages/keras/engine/training.py\", line 612, in data_generator_task\n",
      "    generator_output = next(self._generator)\n",
      "StopIteration\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2933/2933 [==============================] - 1125s - loss: 0.8994  \n",
      "Validation: loss = 0.90192, P = 0.12915, R = 0.09647, F = 0.09806\n",
      "Shuffling training data\n"
     ]
    }
   ],
   "source": [
    "steps_per_epoch = num_rows // batch_size\n",
    "epochs = 5\n",
    "\n",
    "print(\"Batch size: %d\" % batch_size)\n",
    "print(\"Training examples: %d\" % num_rows)\n",
    "print(\"Batches per epoch: %d\" % steps_per_epoch)\n",
    "\n",
    "print(\"Initial validation: loss = %0.5f, P = %0.5f, R = %0.5f, F = %0.5f\"\n",
    "      % validate(model))\n",
    "\n",
    "for epoch in range(epochs):\n",
    "  \n",
    "  def train_data_generator():\n",
    "    for chunk in np.array_split(index_train, steps_per_epoch):\n",
    "      orders_data, reorders_data, user_ids, order_sizes, reorder_sizes, labels = \\\n",
    "        make_training_data(chunk)\n",
    "      yield ({'order_input': orders_data,\n",
    "              'reorder_input': reorders_data,\n",
    "              'user_input': user_ids,\n",
    "              'order_sizes_input': order_sizes,\n",
    "              'reorder_sizes_input': reorder_sizes},\n",
    "             {'output': labels})\n",
    "    return\n",
    "\n",
    "  # Train the model on the entire training set\n",
    "  print(\"Starting epoch %d\" % epoch)\n",
    "  #print(\"Learning rate: %0.5f\" %\n",
    "  #      K.eval(model.optimizer.lr * (1. / (1. + model.optimizer.decay * model.optimizer.iterations))))\n",
    "  \n",
    "#  model.fit_generator(\n",
    "#    train_data_generator(), steps_per_epoch=steps_per_epoch, epochs=1, max_q_size=1,\n",
    "#    class_weight=class_weights)\n",
    "\n",
    "  model.fit_generator(\n",
    "    train_data_generator(), steps_per_epoch=steps_per_epoch, epochs=1, max_q_size=1)\n",
    "  \n",
    "  # Score it against the validation set\n",
    "  print(\"Validation: loss = %0.5f, P = %0.5f, R = %0.5f, F = %0.5f\"\n",
    "        % validate(model))\n",
    "  \n",
    "  # Shuffle the training data (actually just the indices) for next epoch\n",
    "  print(\"Shuffling training data\")\n",
    "  np.random.shuffle(index_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "* Add dropout over the input items, as in the DAN paper\n",
    "\n",
    "\"Deep Unordered Composition Rivals Syntactic Methods for Text Classification\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remaining work\n",
    "\n",
    "There are a few outstanding TODOs that would make this training loop more useful on real work:\n",
    "\n",
    "* Make the generator 'greedier' by using multiple workers and a longer queue, this should speed it up\n",
    "* See if it's possible to push some of the work of `make_training_data` into TensorFlow on the GPU instead\n",
    "* Save the model to disk after every epoch\n",
    "* Terminate training early if validation F-measure stagnates\n",
    "* Reduce the learning rate manually if the training loss stagnates\n",
    "* Test on a held-out set from the end of the time period (see notes at top of file)\n",
    "\n",
    "Also, the process of experimenting with the model structure or constants (size of layers, dropout rate etc.), or the optimizer parameters, is pretty tedious to do by hand.\n",
    "\n",
    "In real life, you'd want to write a hyperparameter tuning script that automatically generates a bunch of different model variants, trains them, tests them on the same validation data, and reports the results. You can do this in parallel over multiple servers to speed up the process."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
