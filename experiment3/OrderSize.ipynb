{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reorder size prediction\n",
    "\n",
    "From a given order, how well can we predict the number of items in it which will be reordered next time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import shutil\n",
    "import os\n",
    "import time\n",
    "from itertools import *\n",
    "from collections import defaultdict\n",
    "import keras as k\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "csv_dir = '../csv/'\n",
    "h5_dir = 'h5/'\n",
    "\n",
    "path = os.path.join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV parsing\n",
    "\n",
    "First read the CSV files into Pandas. See the competition website for descriptions of these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "order_products_prior = pd.read_csv(path(csv_dir, 'order_products__prior.csv'), engine='c',\n",
    "                                   dtype={'order_id':np.int32, \n",
    "                                          'product_id':np.int32, \n",
    "                                          'add_to_cart_order':np.int8, \n",
    "                                          'reordered':np.int8})\n",
    "\n",
    "order_products_train = pd.read_csv(path(csv_dir, 'order_products__train.csv'), engine='c',\n",
    "                                   dtype={'order_id':np.int32, \n",
    "                                          'product_id':np.int32, \n",
    "                                          'add_to_cart_order':np.int8, \n",
    "                                          'reordered':np.int8})\n",
    "\n",
    "orders = pd.read_csv(path(csv_dir, 'orders.csv'), engine='c',\n",
    "                     dtype={'order_id':np.int32,\n",
    "                            'user_id':np.int32,\n",
    "                            'order_number':np.int8,\n",
    "                            'order_dow':np.int8,\n",
    "                            'order_hour_of_day':np.int8\n",
    "                           })\n",
    "\n",
    "products = pd.read_csv(path(csv_dir, 'products.csv'), engine='c',\n",
    "                       dtype={'product_id':np.int32,\n",
    "                              'aisle_id':np.int8,\n",
    "                              'department_id':np.int8\n",
    "                             })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "\n",
    "Now we need to process the data to:\n",
    "\n",
    "* Count the number of users and products, and the size of the biggest order\n",
    " * The model needs to know how big to make its inputs and embedding tables\n",
    "* Filter out each user's first order, for now\n",
    " * None of the items in those are reorders (by definition) so they make things harder for the model\n",
    " * NB this means we can't make predictions based on just one order, which is a tradeoff worth revisiting later\n",
    "* Retrieve pairs of consecutive order IDs for the same user\n",
    " * We're trying to predict the **number of** reorders in the second from the contents of the first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_product_id = max(products.product_id)\n",
    "max_product_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_user_id = max(orders.user_id)\n",
    "max_user_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`order_products` tells us what products were present in each order, and which of those were reorders. It also contains an `add_to_cart_order` column that we're ignoring for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "order_products = pd.concat([order_products_train, order_products_prior], axis=0\n",
    "                          )[['order_id', 'product_id', 'add_to_cart_order']]\n",
    "order_products.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to generate the order pairs for each user. This will also take a few minutes.\n",
    "\n",
    "Some of the later orders will be in the Kaggle test set, which we're not using for this project (as our objective is slightly different from the Kaggle contest). So we filter down to only ones that we have data for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# From the itertools docs -- get consecutive pairs from a sequence\n",
    "def pairwise(iterable):\n",
    "  \"s -> (s0, s1), (s1, s2), (s2, s3), ...\"\n",
    "  a, b = tee(iterable)\n",
    "  next(b, None)\n",
    "  return izip(a, b)\n",
    "\n",
    "def make_x_y(order_ids):\n",
    "  pairs = list(pairwise(order_ids))\n",
    "  return pd.DataFrame.from_records(pairs, columns=['prev', 'next'])\n",
    "\n",
    "order_pairs = orders.sort_values(['user_id', 'order_number']).groupby(['user_id']).order_id.apply(make_x_y)\n",
    "order_pairs.reset_index(inplace=True)\n",
    "del order_pairs['level_1']\n",
    "order_pairs.set_index('prev', inplace=True)\n",
    "order_pairs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training data is ordered, zero-padded product IDs. And also user IDs and size for each order (as a fraction of biggest order size).\n",
    "\n",
    "Labels is the fraction of IDs from *that* order that are present in the next one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "products_per_order = order_products.sort_values(\n",
    "  by=['order_id', 'add_to_cart_order']).groupby(['order_id'], sort=False, group_keys=False).product_id.apply(list)\n",
    "\n",
    "products_per_order.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_rows = len(order_pairs)\n",
    "biggest_order_size = max(products_per_order.apply(len))\n",
    "(num_rows, biggest_order_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "order_matrix = np.zeros((num_rows, biggest_order_size), dtype=np.uint16)\n",
    "order_sizes = np.zeros(num_rows, dtype=np.float32)\n",
    "user_ids = np.zeros(num_rows, dtype=np.uint32)\n",
    "labels = np.zeros(num_rows, dtype=np.float32)\n",
    "\n",
    "row_idx = 0\n",
    "for order_id in order_pairs.index:\n",
    "  product_ids = products_per_order.loc[order_id]\n",
    "  order_matrix[row_idx, :len(product_ids)] = product_ids\n",
    "  user_ids[row_idx] = order_pairs.loc[order_id].user_id\n",
    "  next_order_id = order_pairs.loc[order_id].next\n",
    "  if next_order_id in products_per_order.index:\n",
    "    next_product_ids = products_per_order.loc[next_order_id] \n",
    "  else:\n",
    "    next_product_ids = []\n",
    "  count = len(product_ids)\n",
    "  order_sizes[row_idx] = float(count) / biggest_order_size\n",
    "  next_count = len(set(product_ids).intersection(next_product_ids))\n",
    "  labels[row_idx] = float(next_count) / count\n",
    "  row_idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del products_per_order\n",
    "del order_pairs\n",
    "del order_products\n",
    "del products\n",
    "del orders\n",
    "del order_products_train\n",
    "del order_products_prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ZeroMaskedEntries(k.engine.topology.Layer):\n",
    "    \"\"\"\n",
    "    This layer is called after an Embedding layer.\n",
    "    It zeros out all of the masked-out embeddings.\n",
    "    It also swallows the mask without passing it on.\n",
    "    You can change this to default pass-on behavior as follows:\n",
    "\n",
    "    def compute_mask(self, x, mask=None):\n",
    "        if not self.mask_zero:\n",
    "            return None\n",
    "        else:\n",
    "            return K.not_equal(x, 0)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        self.support_mask = True\n",
    "        super(ZeroMaskedEntries, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.output_dim = input_shape[1]\n",
    "        self.repeat_dim = input_shape[2]\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        mask = K.cast(mask, 'float32')\n",
    "        mask = K.repeat(mask, self.repeat_dim)\n",
    "        mask = K.permute_dimensions(mask, (0, 2, 1))\n",
    "        return x * mask\n",
    "\n",
    "    def compute_mask(self, input_shape, input_mask=None):\n",
    "        return None\n",
    "\n",
    "\n",
    "def mask_aware_mean(x):\n",
    "    # recreate the masks - all zero rows have been masked\n",
    "    mask = K.not_equal(K.sum(K.abs(x), axis=2, keepdims=True), 0)\n",
    "\n",
    "    # number of that rows are not all zeros\n",
    "    n = K.sum(K.cast(mask, 'float32'), axis=1, keepdims=False)\n",
    "    \n",
    "    # compute mask-aware mean of x, or all zeroes if no rows present\n",
    "    x_mean = K.sum(x, axis=1, keepdims=False) / n\n",
    "    x_mean = tf.where(tf.is_nan(x_mean), tf.zeros_like(x_mean), x_mean)\n",
    "\n",
    "    return x_mean\n",
    "\n",
    "\n",
    "def mask_aware_mean_output_shape(input_shape):\n",
    "    shape = list(input_shape)\n",
    "    assert len(shape) == 3 \n",
    "    return (shape[0], shape[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants for both models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Layer size constants\n",
    "user_embedding_size = 50\n",
    "#product_embedding_size = 50\n",
    "\n",
    "# TODO Dropout rate\n",
    "dropout = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DAN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Inputs\n",
    "\n",
    "#user_input = k.layers.Input(shape=(1,), name='user_input')\n",
    "\n",
    "dan_order_input = k.layers.Input(shape=(biggest_order_size,), name='order_input')\n",
    "\n",
    "#order_size_input = k.layers.Input(shape=(1,), name='order_size_input')\n",
    "\n",
    "# Embeddings\n",
    "\n",
    "dan_product_embedding = k.layers.Embedding(\n",
    "  input_dim=max_product_id + 1,\n",
    "  output_dim=product_embedding_size,\n",
    "  mask_zero=True,\n",
    "  name='product_embedding')\n",
    "\n",
    "#user_embedding = k.layers.Embedding(\n",
    "#  input_dim=max_user_id + 1, output_dim=user_embedding_size, name='user_embedding')\n",
    "\n",
    "dan_order_embeddings = dan_product_embedding(dan_order_input)\n",
    "\n",
    "# The DAN itself\n",
    "\n",
    "dan_order_vector = k.layers.Lambda(\n",
    "  mask_aware_mean, mask_aware_mean_output_shape, name='mean')(dan_order_embeddings)\n",
    "\n",
    "dan_order_hidden_1 = k.layers.Dense(\n",
    "  product_embedding_size, activation='relu', name='order_hidden_1')(dan_order_vector)\n",
    "\n",
    "dan_order_hidden_2 = k.layers.Dense(\n",
    "  product_embedding_size, activation='relu', name='order_hidden_2')(dan_order_hidden_1)\n",
    "\n",
    "# Output -- no activation\n",
    "\n",
    "dan_output = k.layers.Dense(1, name='output')(dan_order_hidden_2)\n",
    "\n",
    "# Compile the model\n",
    "\n",
    "dan_model = k.models.Model(\n",
    "  inputs=[dan_order_input],\n",
    "  outputs=dan_output)\n",
    "\n",
    "dan_model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "dan_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "dan_model.fit(order_matrix, labels,\n",
    "              batch_size=batch_size,\n",
    "              validation_split=0.01)\n",
    "\n",
    "duration = time.time() - start\n",
    "print('Finished training in %d secs' % duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Inputs\n",
    "\n",
    "gru_order_input = k.layers.Input(shape=(biggest_order_size,), name='order_input')\n",
    "\n",
    "# Embeddings\n",
    "\n",
    "gru_product_embedding = k.layers.Embedding(\n",
    "  input_dim=max_product_id + 1,\n",
    "  output_dim=product_embedding_size,\n",
    "  mask_zero=True,\n",
    "  name='product_embedding')\n",
    "\n",
    "gru_order_embeddings = gru_product_embedding(gru_order_input)\n",
    "\n",
    "# The GRU itself\n",
    "\n",
    "order_gru = k.layers.GRU(\n",
    "  product_embedding_size * 2, name='order_gru')(gru_order_embeddings)\n",
    "\n",
    "# Output -- no activation\n",
    "\n",
    "gru_output = k.layers.Dense(1, name='output')(order_gru)\n",
    "\n",
    "# Compile the model\n",
    "\n",
    "gru_model = k.models.Model(\n",
    "  inputs=[gru_order_input],\n",
    "  outputs=gru_output)\n",
    "\n",
    "gru_model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "gru_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "gru_model.fit(order_matrix, labels,\n",
    "              batch_size=batch_size,\n",
    "              validation_split=0.01)\n",
    "\n",
    "duration = time.time() - start\n",
    "print('Finished training in %d secs' % duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Junk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shuffle(orders_in, sizes_in, users_in, labels_in):\n",
    "  n = len(orders_in)\n",
    "  assert len(sizes_in) == n\n",
    "  assert len(users_in) == n\n",
    "  assert len(labels_in) == n\n",
    "  p = np.random.permutation(n)\n",
    "  return orders_in[p], sizes_in[p], users_in[p], labels_in[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "num_valid = 5000\n",
    "\n",
    "index = np.arange(num_rows, dtype=np.uint32)\n",
    "np.random.shuffle(index)\n",
    "index_valid = index[:num_valid]\n",
    "index_train = index[num_valid:]\n",
    "\n",
    "order_matrix_valid = order_matrix[index_valid, :]\n",
    "order_sizes_valid = order_sizes[index_valid]\n",
    "user_ids_valid = user_ids[index_valid]\n",
    "labels_valid = labels[index_valid]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
